1.JDK和JRE的区别

2.java的基本数据类型
    数据类型	大小
    byte(字节)	1个字节(8位)
    shot(短整型) 	2个字节(16位)
    int(整型)	4个字节(32位)
    long(长整型) 	8个字节(32位)
    float(浮点型)	4个字节(32位)
    double(双精度)	8个字节(64位)
    char(字符型)	2个字节(16位)
    boolean(布尔型)	1位
    附加：
     String是基本数据类型吗?(String不是基本数据类型)
     String的长度是多少，有限制?(长度受内存大小的影响)

进程的三种状态:
    进程执行时的间断性决定了进程可能具有多种状态。
    进程具有三种基本状态:
    1、就绪状态。某些进程“万事俱备”(必要资源)，只差CPU。（就绪队列）
    2、执行状态。某进程占有CPU并在CPU上执行其程序。
    3、阻塞状态。某些进程由于某种原因不能继续运行下去，等待处 理问题。也称为等待状态或封锁状态。如：请求I/O。（多个等待队列）
    三种状态随着执行和条件的变化而发生转换

3.创建线程的方式
    浅谈三种方式优劣势
    通过继承Thread类或实现Runnable、Callable接口都可以实现多线程，不过实现Runnable接口与实现Callable接口的方式基本相同，只是Callable接口里定义的方法有返回值，可以声明抛出异常而已。因此可以将实现Runnable接口和实现Callable接口归为一种方式。这种方式与继承Thread方式之间的主要差别如下。
    1.采用实现Runnable、Callable接口的方式创建多线程的优缺点：
优势：（1）线程类只是实现了Runnable接口与Callable接口，还可以继承其他类。
     （2）在这种方式下，多个线程可以共享一个target对象，所以非常适合多个相同线程来处理同一份资源的情况，从而可以将CPU、代码和数据分开，形成清晰的模型，较好地体现了面向对象的思想。
劣势：编程稍稍复杂，如果需要访问当前线程，则必须使用Thread.currentThread（）方法。
    2.采用继承Thread类的方法创建多线程的优缺点：
劣势：因为线程类已经继承了Thread类，所以不能再继承其他父类。
优势：编写简单，如果需要访问当前线程，则无须使用Thread.currentThread（）方法，直接使用this即可获得当前线程。
五、总结
    鉴于上面分析，因此一般推荐采用实现Runnable接口、Callable接口的方式来创建多线程。

4.什么是死锁
    线程死锁是指由于两个或者多个线程互相持有对方所需要的资源，导致这些线程处于等待状态，无法前往执行
    当线程进入对象的synchronized代码块时，便占有了资源，直到它退出该代码块或者调用wait方法，才释放资源，在此期间，其他线程将不能进入该代码块。
    当线程互相持有对方所需要的资源时，会互相等待对方释放资源，如果线程都不主动释放所占有的资源，将产生死锁
   「如果一组进程中的每个进程都在等待一个事件，而这个事件只能由该组中的另一个进程触发，这种情况会导致死锁」

当然死锁的产生是必须要满足一些特定条件的：
    1.互斥条件：进程对于所分配到的资源具有排它性，即一个资源只能被一个进程占用，直到被该进程释放
    2.请求和保持条件：进程已经保持了至少一个资源，但又提出了新的资源请求，而该资源已被其他进程占有，此时请求进程被阻塞，但对自己已获得的资源保持不放。
    3.不剥夺条件：进程所获得的资源在未使用完毕之前，不能被其他进程强行夺走，即只能由获得该资源的进程自己来释放（只能是主动释放)。
    4.循环等待条件：当发生死锁时，所等待的进程必定会形成一个环路（类似于死循环），造成永久阻塞。
预防死锁的方法：
    1、破坏“互斥”条件:
        就是在系统里取消互斥。若资源不被一个进程独占使用，那么死锁是肯定不会发生的。但一般来说在所列的四个条件中，“互斥”条件是无法破坏的。因此，在死锁预防里主要是破坏其他几个必要条件，而不去涉及破坏“互斥”条件。
        注意：互斥条件不能被破坏，否则会造成结果的不可再现性。
    2、破坏“占有并等待”条件:
        破坏“占有并等待”条件，就是在系统中不允许进程在已获得某种资源的情况下，申请其他资源。即要想出一个办法，阻止进程在持有资源的同时申请其他资源。
        方法一：创建进程时，要求它申请所需的全部资源，系统或满足其所有要求，或什么也不给它。这是所谓的 “ 一次性分配”方案。
        方法二：要求每个进程提出新的资源申请前，释放它所占有的资源。这样，一个进程在需要资源S时，须先把它先前占有的资源R释放掉，然后才能提出对S的申请，即使它可能很快又要用到资源R。
    3、破坏“不可抢占”条件：
        破坏“不可抢占”条件就是允许对资源实行抢夺。
        方法一：如果占有某些资源的一个进程进行进一步资源请求被拒绝，则该进程必须释放它最初占有的资源，如果有必要，可再次请求这些资源和另外的资源。
        方法二：如果一个进程请求当前被另一个进程占有的一个资源，则操作系统可以抢占另一个进程，要求它释放资源。只有在任意两个进程的优先级都不相同的条件下，方法二才能预防死锁。
    4、破坏“循环等待”条件：
        破坏“循环等待”条件的一种方法，是将系统中的所有资源统一编号，进程可在任何时刻提出资源申请，但所有申请必须按照资源的编号顺序（升序）提出。这样做就能保证系统不出现死锁。

避免死锁的方法：
    1、有序资源分配法
        这种算法资源按某种规则系统中的所有资源统一编号（例如打印机为1、磁带机为2、磁盘为3、等等），申请时必须以上升的次序。
    2、单个资源的银行家算法



5.java中class.forName和classLoader加载类的区分
    总结
        ClassLoader.load()方法加载一个类的时候不会触发这个类初始化（有关类初始化的相关内容请查看我的另外一篇详细的介绍）
        自然static静态代码块就不会被执行
        Class.forName()方法加载一个类之后会强制这个类初始化
     java中class.forName和classLoader都可用来对类进行加载。
     前者除了将类的.class文件加载到jvm中之外，还会对类进行解释，执行类中的static块。
     而classLoader只干一件事情，就是将.class文件加载到jvm中，不会执行static中的内容,只有在newInstance才会去执行static块。Class.forName(name, initialize, loader)带参函数也可控制是否加载static块。并且只有调用了newInstance()方法采用调用构造函数，创建类的对象

        下面说一下两者具体的执行过程
    1.LoadClass（）方法加载类及初始化过程：
        类加载（loadclass（））（加载）——》newInstance（）（链接+初始化）
        newInstance（）:
        （开始连接）静态代码块——》普通变量分配准备（a=0;b=0;c=null）——》（开始初始化）普通变量赋值（a=1;b=2;c=”haha”）——》构造方法——》初始化成功。

    2.Class.forName(Stirng className)一个参数方法加载类及初始化过程：
        类加载(Class.forName())（加载）——》静态代码块——》newInstance（）（链接+初始化）

    newInstance（）：
        （开始连接）普通变量分配准备（a=0;b=0;c=null）——》（开始初始化）普通变量赋值（a=1;b=2;c=”haha”）——》构造方法——》初始化成功。


6.谈谈RDD
    1为什么会产生RDD
（1） 传统的MapReduce虽然具有自动容错、平衡负载和可拓展性的优点，但是其最大缺点是采用非循环式的数据流模型，使得在迭代计算式要进行大量的磁盘IO操作。RDD正是解决这一缺点的抽象方法。
（2） RDD是Spark提供的最重要的抽象的概念，它是一种有容错机制的特殊集合，可以分布在集群的节点上，以函数式编程操作集合的方式，进行各种并行操作。
     可以将RDD理解为一个具有容错机制的特殊集合，它提供了一种只读、只能由已存在的RDD转换而来的共享内存，然后将所有数据都加载到内存中，方便进行多次重用。
     rdd更多的是一个逻辑概念，我们对于rdd的操作最终会映射到内存或者磁盘当中，也就是操作rdd通过映射就等同于操作内存或者磁盘。

    a. 他是分布式的，可以分布在多台机器上，进行计算。
    b. 他是弹性的，计算过程中内存不够时它会和磁盘进行数据交换。
    c. 这些限制可以极大的降低自动容错开销
    d. 实质是一种更为通用的迭代并行计算框架
    （1）为什么会有Spark？因为传统的并行计算模型无法有效的解决迭代计算（iterative）和交互式计算（interactive）；而Spark的使命便是解决这两个问题，这也是他存在的价值和理由。
    （2）Spark如何解决迭代计算？其主要实现思想就是RDD，把所有计算的数据保存在分布式的内存中。迭代计算通常情况下都是对同一个数据集做反复的迭代计算，数据在内存中将大大提升IO操作。这也是Spark涉及的核心：内存计算。
    （3）Spark如何实现交互式计算？因为Spark是用scala语言实现的，Spark和scala能够紧密的集成，所以Spark可以完美的运用scala的解释器，使得其中的scala可以向操作本地集合对象一样轻松操作分布式数据集。
        （4）Spark和RDD的关系？可以理解为：RDD是一种具有容错性基于内存的集群计算抽象方法，Spark则是这个抽象方法的实现。

RDD的五大特性：
    1 、A list of partitions
      --RDD是由多个partition构成的。
    2、A function for computing each split
      --RDD的每个分区上都有一个函数去作用
    3、 A list of dependencies on other RDDs
      --RDD有依赖性，通常情况下一个RDD是来源于另一个RDD，这个叫做lineage。RDD会记录下这些依赖，方便容错。
    4、Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)
      --可选项，如果RDD里面存的数据是key-value形式，则可以传递一个自定义的Partitioner进行重新分区，例如这里自定义的Partitioner是基于key进行分区，那则会将不同RDD里面的相同key的数据放到同一个partition里面。
    5、Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)
    最优的位置去计算，也就是数据的本地性。

RDD详解：
      在集群背后，有一个非常重要的分布式数据架构，即弹性分布式数据集（Resilient Distributed Dataset，RDD）。
      RDD是Spark的最基本抽象,是对分布式内存的抽象使用
      实现了以操作本地集合的方式来操作分布式数据集的抽象实现。
      RDD是Spark最核心的东西，它表示已被分区，不可变的并能够被并行操作的数据集合，不同的数据集格式对应不同的RDD实现。
      RDD必须是可序列化的。RDD可以cache到内存中，每次对RDD数据集的操作之后的结果，都可以存放到内存中
      下一个操作可以直接从内存中输入，省去了MapReduce大量的磁盘IO操作。
      这对于迭代运算比较常见的机器学习算法, 交互式数据挖掘来说，效率提升比较大。

     （1）RDD的特点
      1）创建：只能通过转换 ( transformation ，如map/filter/groupBy/join 等，区别于动作 action) 从两种数据源中创建 RDD 1 ）稳定存储中的数据； 2 ）其他 RDD。
      2）只读：状态不可变，不能修改。
      3）分区：支持使 RDD 中的元素根据那个 key 来分区 ( partitioning ) ，保存到多个结点上。还原时只会重新计算丢失分区的数据，而不会影响整个系统。
      4）路径：在 RDD 中叫世族或血统 ( lineage ) ，即 RDD 有充足的信息关于它是如何从其他 RDD 产生而来的。
      5）持久化：支持将会被重用的 RDD 缓存 ( 如 in-memory 或溢出到磁盘 )。
      6）延迟计算： Spark 也会延迟计算 RDD ，使其能够将转换管道化 (pipeline transformation)。
      7）操作：丰富的转换（transformation）和动作 ( action ) ， count/reduce/collect/save 等。
      执行了多少次transformation操作，RDD都不会真正执行运算（记录lineage），只有当action操作被执行时，运算才会触发。

     （2）RDD的好处
      1）RDD只能从持久存储或通过Transformations操作产生，相比于分布式共享内存(DSM)可以更高效实现容错，对于丢失部分数据分区只需根据它的lineage就可重新计算出来，而不需要做特定的Checkpoint。
      2）RDD的不变性，可以实现类Hadoop MapReduce的推测式执行。
      3）RDD的数据分区特性，可以通过数据的本地性来提高性能，这不Hadoop MapReduce是一样的。
      4）RDD都是可序列化的，在内存不足时可自动降级为磁盘存储，把RDD存储于磁盘上，这时性能会有大的下降但不会差于现在的MapReduce。
      5）批量操作：任务能够根据数据本地性 (data locality) 被分配，从而提高性能。

     （3）RDD的内部属性
      通过RDD的内部属性，用户可以获取相应的元数据信息。通过这些信息可以支持更复杂的算法或优化。
      1）分区列表：通过分区列表可以找到一个RDD中包含的所有分区及其所在地址。
      2）计算每个分片的函数：通过函数可以对每个数据块进行RDD需要进行的用户自定义函数运算。
      3）对父RDD的依赖列表，依赖还具体分为宽依赖和窄依赖，但并不是所有的RDD都有依赖。
      4）可选：key-value型的RDD是根据哈希来分区的，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce。
      5）可选：每一个分片的优先计算位置（preferred locations），比如HDFS的block的所在位置应该是优先计算的位置。(存储的是一个表，可以将处理的分区“本地化”)

Spark内存动态调整：
    Execution 内存	主要用于存放 Shuffle、Join、Sort、Aggregation 等计算过程中的临时数据
    Storage 内存	主要用于存储 spark 的 cache 数据，例如RDD的缓存、unroll数据
    用户内存（User Memory）	主要用于存储 RDD 转换操作所需要的数据，例如 RDD 依赖等信息
    预留内存（Reserved Memory）	系统预留内存，会用来存储Spark内部对象
    动态调整策略
    具体的实现逻辑如下：
    程序提交的时候我们都会设定基本的 Execution 内存和 Storage 内存区域（通过 spark.memory.storageFraction 参数设置）；
        在程序运行时，双方的空间都不足时，则存储到硬盘；
        将内存中的块存储到磁盘的策略是按照 LRU 规则(Least Recently Used)进行的。
        若己方空间不足而对方空余时，可借用对方的空间;（存储空间不足是指不足以放下一个完整的 Block）
        Execution 内存的空间被对方占用后，可让对方将占用的部分转存到硬盘，然后”归还”借用的空间
        Storage 内存的空间被对方占用后，目前的实现是无法让对方”归还”，因为需要考虑 Shuffle 过程中的很多因素，实现起来较为复杂；
        而且 Shuffle 过程产生的文件在后面一定会被使用到，而 Cache 在内存的数据不一定在后面使用。
    Execution内存被对方占用可以强制收回，保证shuffle等计算，Storage内存不可以被收回
    注意，上面说的借用对方的内存需要借用方和被借用方的内存类型都一样，都是堆内内存或者都是堆外内存，不存在堆内内存不够去借用堆外内存的空间。
Spark中Task内存申请流程
    为了更好地使用使用内存，Executor 内运行的 Task 之间共享着 Execution 内存。
    具体的，Spark 内部维护了一个 HashMap 用于记录每个 Task 占用的内存：
    当 Task 需要在 Execution 内存区域申请 numBytes 内存，其先判断 HashMap 里面是否维护着这个 Task 的内存使用情况，如果没有，则将这个 Task 内存使用置为0，并且以 TaskId 为 key，内存使用为 value 加入到 HashMap 里面。
    之后为这个 Task 申请 numBytes 内存，如果 Execution 内存区域正好有大于 numBytes 的空闲内存，则在 HashMap 里面将当前 Task 使用的内存加上 numBytes，然后返回；如果当前 Execution 内存区域无法申请到每个 Task 最小可申请的内存，则当前 Task 被阻塞，直到有其他任务释放了足够的执行内存，该任务才可以被唤醒。
    每个 Task 可以使用 Execution 内存大小范围为 1/2N ~ 1/N，其中 N 为当前 Executor 内正在运行的 Task 个数。
    一个 Task 能够运行必须申请到最小内存为 (1/2N * Execution 内存)；当 N = 1 的时候，Task 可以使用全部的 Execution 内存。比如如果 Execution 内存大小为 10GB，当前 Executor 内正在运行的 Task 个数为5，则该 Task 可以申请的内存范围为 10 / (2 * 5) ~ 10 / 5，也就是 1GB ~ 2GB的范围。


问：Java 线程优先级是怎么定义的，Java 线程有几种状态？
    答：Java 线程的优先级定义为从 1 到 10 的等级，默认为 5，设置和获取线程优先级的方法是 setPriority(int newPriority) 和 getPriority()
    Java 的这个优先级会被映射到操作系统中线程的优先级，不过由于操作系统各不相同，不一定都是 10 个优先级
    所以 Java中不同的优先级可能会被映射到操作系统中相同的优先级，同时优先级对操作系统而言更多的是一种建议和提示而非强制，所以我们不要过于依赖优先级。
    Java Thread 可以通过 State getState() 来获取线程状态，Thread.State 枚举定义了 NEW、RUNNABLE、BLOCKED、WAITING、TIMED_WAITING、TERMINATED 六种线程状态，其实真正严格意义来说线程只有就绪、阻塞、运行三种状态，Java 线程之所以有六种状态其实是站在 Thread 对象实例的角度来看待的，具体解释如下：
    NEW（新建），表示线程 Thread 刚被创建，还没调用 start 方法。
    RUNNABLE（运行，实质对应就绪和运行状态），表示 Thread 线程正在 JVM 中运行，也就是说处于就绪和运行状态的线程在 Thread 中都表现为 RUNNABLE。
    BLOCKED（阻塞，实质对应阻塞状态），表示等待监视锁可以重新进行同步代码块中执行，此线程需要获得某个锁才能继续执行，而这个锁目前被其他线程持有，所以进入了被动的等待状态，直到抢到了那个锁才会再次进入就绪状态。处于受阻塞状态的某一线程正在等待监视器锁，以便进入一个同步的块或方法，或者在调用 wait 之后再次进入同步的块或方法。
    WAITING（等待，实质对应阻塞状态），表示此线程正处于无限期的主动等待中，直到有人唤醒它它才会再次进入就绪状态。某一线程因为调用下不带超时值的 wait、不带超时值的 join、LockSupport.park 会进入等待状态，处于等待状态的线程正等待另一个线程以执行特定操作，例如已经在某一对象上调用了 Object.wait() 的线程正等待另一个线程以便在该对象上调用 Object.notify() 或 Object.notifyAll()，或者已经调用了 Thread.join() 的线程正在等待指定线程终止。
    TIMED_WAITING（有限等待，实质对应阻塞状态），表示此线程正处于有限期的主动等待中，要么有人唤醒它，要么等待够了一定时间之后才会再次进入就绪状态，譬如调用带有超时的 sleep、join、wait 方法可能导致线程处于等待状态。
    TERMINATED（终止），表示线程执行完毕，已经退出。

1. 初始(NEW)：新创建了一个线程对象，但还没有调用start()方法。
2. 运行(RUNNABLE)：Java线程中将就绪（ready）和运行中（running）两种状态笼统的称为“运行”。
线程对象创建后，其他线程(比如main线程）调用了该对象的start()方法。该状态的线程位于可运行线程池中，等待被线程调度选中，获取CPU的使用权，此时处于就绪状态（ready）。就绪状态的线程在获得CPU时间片后变为运行中状态（running）。
3.阻塞(BLOCKED)：表示线程阻塞于锁。
4.等待(WAITING)：进入该状态的线程需要等待其他线程做出一些特定动作（通知或中断）。
5.超时等待(TIMED_WAITING)：该状态不同于WAITING，它可以在指定的时间后自行返回。
6. 终止(TERMINATED)：表示该线程已经执行完毕。

Java中线程的状态分为6种。
    1. 初始(NEW)：新创建了一个线程对象，但还没有调用start()方法。
    2. 运行(RUNNABLE)：Java线程中将就绪（ready）和运行中（running）两种状态笼统的称为“运行”。
    线程对象创建后，其他线程(比如main线程）调用了该对象的start()方法。该状态的线程位于可运行线程池中，等待被线程调度选中，获取CPU的使用权，此时处于就绪状态（ready）。就绪状态的线程在获得CPU时间片后变为运行中状态（running）。
    3.阻塞(BLOCKED)：表示线程阻塞于锁。
    4.等待(WAITING)：进入该状态的线程需要等待其他线程做出一些特定动作（通知或中断）。
    5.超时等待(TIMED_WAITING)：该状态不同于WAITING，它可以在指定的时间后自行返回。
    6. 终止(TERMINATED)：表示该线程已经执行完毕。

线程的状态图
    1. 初始状态
        实现Runnable接口和继承Thread可以得到一个线程类，new一个实例出来，线程就进入了初始状态。

    2.1. 就绪状态
        就绪状态只是说你资格运行，调度程序没有挑选到你，你就永远是就绪状态。
        调用线程的start()方法，此线程进入就绪状态。
        当前线程sleep()方法结束，其他线程join()结束，等待用户输入完毕，某个线程拿到对象锁，这些线程也将进入就绪状态。
        当前线程时间片用完了，调用当前线程的yield()方法，当前线程进入就绪状态。
        锁池里的线程拿到对象锁后，进入就绪状态。
    2.2. 运行中状态
        线程调度程序从可运行池中选择一个线程作为当前线程时线程所处的状态。这也是线程进入运行状态的唯一一种方式。

    3. 阻塞状态
        阻塞状态是线程阻塞在进入synchronized关键字修饰的方法或代码块(获取锁)时的状态。

    4. 等待
        处于这种状态的线程不会被分配CPU执行时间，它们要等待被显式地唤醒，否则会处于无限期等待的状态。

    5. 超时等待
        处于这种状态的线程不会被分配CPU执行时间，不过无须无限期等待被其他线程显示地唤醒，在达到一定时间后它们会自动唤醒。

    6. 终止状态
        当线程的run()方法完成时，或者主线程的main()方法完成时，我们就认为它终止了。这个线程对象也许是活的，但是，它已经不是一个单独执行的线程。线程一旦终止了，就不能复生。
        在一个终止的线程上调用start()方法，会抛出java.lang.IllegalThreadStateException异常。
    等待队列
    调用obj的wait(), notify()方法前，必须获得obj锁，也就是必须写在synchronized(obj) 代码段内。
与等待队列相关的步骤和图
    1.线程1获取对象A的锁，正在使用对象A。
    2.线程1调用对象A的wait()方法。
    3.线程1释放对象A的锁，并马上进入等待队列。
    4.锁池里面的对象争抢对象A的锁。
    5.线程5获得对象A的锁，进入synchronized块，使用对象A。
    6.线程5调用对象A的notifyAll()方法，唤醒所有线程，所有线程进入同步队列。若线程5调用对象A的notify()方法，则唤醒一个线程，不知道会唤醒谁，被唤醒的那个线程进入同步队列。
    7.notifyAll()方法所在synchronized结束，线程5释放对象A的锁。
    8.同步队列的线程争抢对象锁，但线程1什么时候能抢到就不知道了。
同步队列状态
    当前线程想调用对象A的同步方法时，发现对象A的锁被别的线程占有，此时当前线程进入同步队列。简言之，同步队列里面放的都是想争夺对象锁的线程。
    当一个线程1被另外一个线程2唤醒时，1线程进入同步队列，去争夺对象锁。
    同步队列是在同步的环境下才有的概念，一个对象对应一个同步队列。
几个方法的比较
    1.Thread.sleep(long millis)，一定是当前线程调用此方法，当前线程进入TIMED_WAITING状态，但不释放对象锁，millis后线程自动苏醒进入就绪状态。作用：给其它线程执行机会的最佳方式。
    2.Thread.yield()，一定是当前线程调用此方法，当前线程放弃获取的CPU时间片，但不释放锁资源，由运行状态变为就绪状态，让OS再次选择线程。作用：让相同优先级的线程轮流执行，但并不保证一定会轮流执行。实际中无法保证yield()达到让步目的，因为让步的线程还有可能被线程调度程序再次选中。Thread.yield()不会导致阻塞。该方法与sleep()类似，只是不能由用户指定暂停多长时间。
    3.t.join()/t.join(long millis)，当前线程里调用其它线程t的join方法，当前线程进入WAITING/TIMED_WAITING状态，当前线程不会释放已经持有的对象锁。线程t执行完毕或者millis时间到，当前线程进入就绪状态。
    4.obj.wait()，当前线程调用对象的wait()方法，当前线程释放对象锁，进入等待队列。依靠notify()/notifyAll()唤醒或者wait(long timeout) timeout时间到自动唤醒。
    5.obj.notify()唤醒在此对象监视器上等待的单个线程，选择是任意性的。notifyAll()唤醒在此对象监视器上等待的所有线程。

MYSQL查询过程:
    一、执行一个查询过程概述
        1.客户端发送一条查询给服务器；
        2.服务器先检查查询缓存，如果命中了缓存，则立即返回存储在缓存中的结果。否则进入下一阶段；
        3.服务器端进行sql解析、预处理，再由优化器生成对应的执行计划；
        4.mysql根据优化器生成的执行计划，调用存储引擎的api来执行查询；
        5.将结果返回给客户端；

    二、查询缓存
        查询缓存保存查询返回的完整结构；
        命中查询缓存时，mysql会立即返回结果，跳过解析、优化和执行阶段；
        查询缓存系统会跟踪查询中设计的每个表，如果这些表发生变化，和这个表相关的所有缓存数据都将失效；
        判断缓存是否命中时，不会进行解析查询语句，直接使用sql语句和客户端发送过来的其他原始信息，任何字符上的不同，例如空格、注解等，都会导致缓存不命中；
        当查询语句有一些不确定的数据时，则不会被缓存；
        查询缓存配置参数：
        query_cache_type：是否打开缓存。OFF、ON和DEMAND。DEMAND表示只有在查询语句中明确写明SQL_CACHE的语句才会放入查询缓存。eg：select sql_cache * from table_name;
        query_cache_size：缓存使用的总内存空间大小，单位是字节；
        query_cache_min_res_unit：分配内存块时的最小单位，较小的该值可以减少碎片导致的内存空间浪费，但会导致更频繁的内存块操作；
        query_cache_limit：mysql能够缓存的最大查询结果，如果查询结果大于这个值，则不会被缓存；由于查询缓存在数据生成的时候就开始尝试缓存数据，所以当结果全部返回后，mysql才知道查询结果是否超出限制。超出之后，才会将结果从查询缓存中删除；
        query_cache_wlock_invalidate：如果某个数据表被其他连接锁住，是否仍然从查询缓存中返回结果，默认OFF，表示仍然可以返回；
    三、语法解析器和预处理器
        mysql解析器通过关键字将sql语句进行解析，并生成对应的解析树；
        mysql解析器将使用mysql语法规则验证和解析查询，eg：验证是否使用错误的关键字、使用关键字的顺序是否正确、验证引号是否前后匹配等；
        预处理器根据一些mysql规则进行进一步检查解析树是否合法，eg：检查数据表和数据列是否存在，解析名字和别名是否有歧义；
        下一步预处理器验证用户权限，查看用户是否有操作权限，通常很快；
    四、查询优化器
        优化器的作用就是找到最好的执行计划；
        语法树被认为是合法后，优化器将sql语句转换为执行计划，一条查询可以有多种执行方式，最后都返回相同的结果；
        生成执行计划过程
        耗时较多，特别是存在许多可选的执行计划时；
    在一条sql语句执行过程中将该语句对应的最终执行计划进行缓存，下一次就可以直接使用已缓存的执行计划，从而提高sql语句的执行速度；
    mysql使用基于成本的优化器（CBO cost-based optimizer），会预测一个查询使用某种执行计划的成本，选择其中成本最小的一个；
    优化器会根据优化规则对关系表达式进行转换，经过优化规则后会生成另一个关系表达式，原有表达式也会保留；
    经过一系列转换后会生成多个执行计划，然后CBO会根据统计信息和代价模型（cost model）计算每个执行计划的cost，从中挑选cost最小的执行计划；
    导致mysql优化器选择非最优执行计划的原因
    mysql是根据成本计算得出的最优计划，可能执行时间并不是最短的；
    有时候可能无法估算所有可能的执行计划，导致可能错过最优的执行计划；
    执行计划成本估算不等同于实际执行的成本，mysql层面无法知道哪些页面在内存中，哪些在磁盘上，实际执行过程中需要多少次物理IO无法得知；
    mysql不会考虑不受其控制的操作成本，eg：执行用户自定义函数的成本；
    统计信息不准确，mysql依赖存储引擎提供的统计信息（每个表有多少页面、数据行和索引的长度、索引的分布等）来估计成本，InnoDB因为MVCC的架构，并不能维护一个数据表行数的精确统计信息；
    mysql可以处理的优化类型
    重新定义关联表的顺序；
    使用等价转换原则，eg：移除一些恒成立或恒不成立的判断；
    可能的表达式转换为常数表达式；
    提前终止查询，使用limit；
    五、查询执行引擎
        查询执行引擎根据执行计划来完成整个查询；
        执行计划是一个数据结构（指令树），不是和其他关系型数据库那样生成对应的字节码；
        mysql根据执行计划给出的指令逐步执行，在执行过程中，有大量的操作需要调用存储引擎实现的接口来完成，这些接口即为“handler API”；
        查询中每一个表由一个handler的实例表示。在优化阶段mysql就为每一个表创建了一个handler实例，优化器可以根据这些实例的接口获取表的相关信息，eg：列名、索引、统计信息等；
    六、返回客户端结果
        如果查询可以被缓存，mysql会在这个阶段将结果存放到查询缓存中；
        mysql将结果集返回给客户端是一个增量逐步返回的过程，在查询生成第一条结果时，mysql就可以开始向客户端逐步返回结果了；
        增量逐步返回的好处：服务端无须存储太多的结果，不会因为返回太多的结果而消耗太多内存；同时让客户端第一时间获得返回结果；
        结果集中的每一行都会以一个满足mysql客户端/服务端通信协议的包发送，再通过tcp协议进行传输，传输过程中，可能对mysql的包进行缓存然后批量传输；


说说AQS原理吧
    AQS主要是采用volatile修饰的变量state，通过对state的CAS判断来获取锁和解锁
    如果CAS成功，就获取到锁，如果没有获取成功，线程被挂起
    并且存在等待队列和条件等待队列来park相关线程之后入队等待
    有公平和非公平两者模式来唤醒等待的线程。
    共享式和独占式两种模式
    主要是为了封装和抽象，通过封装了公共的方法，减少重复代码。
    可重入锁：
        可重入锁的意思，就是你可以对一个ReentrantLock对象多次执行lock()加锁和unlock()释放锁，也就是可以对一个锁加多次，叫做可重入加锁。
        其实每次线程1可重入加锁一次，会判断一下当前加锁线程就是自己，那么他自己就可以可重入多次加锁，每次加锁就是把state的值给累加1，别的没啥变化。
    具体原理:
        抽象的队列式同步器，是除了java自带的synchronized关键字之外的锁机制
        AQS的核心思想是，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程并将共享资源设置为锁定状态
        如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制
        这个机制AQS是用CLH队列锁实现的，即将暂时获取不到锁的线程加入到队列中。
        CLH队列是一个虚拟的双向队列，虚拟的双向队列即不存在队列实例，仅存在节点之间的关联关系。
        AQS是将每一条请求共享资源的线程封装成一个CLH锁队列的一个结点（Node），来实现锁的分配
    步骤：
        1.初始阶段这个AQS对象内部有一个核心的变量叫做state，是int类型的，代表了加锁的状态。
          初始状态下，这个state的值是0。
          另外，这个AQS内部还有一个关键变量，用来记录当前加锁的是哪个线程
          初始化状态下，这个变量是null。
        2.接着线程1跑过来调用ReentrantLock的lock()方法尝试进行加锁，这个加锁的过程，直接就是用CAS操作将state值从0变为1。
          如果之前没人加过锁，那么state的值肯定是0，此时线程1就可以加锁成功。
          一旦线程1加锁成功了之后，就可以设置当前加锁线程是自己。
        3.其实每次线程1可重入加锁一次，会判断一下当前加锁线程就是自己，那么他自己就可以可重入多次加锁，每次加锁就是把state的值给累加1，别的没啥变化。
        4.如果线程1加锁了之后，线程2跑过来加锁会怎么样呢？
            我们来看看锁的互斥是如何实现的？线程2跑过来一下看到，哎呀！state的值不是0啊？所以CAS操作将state从0变为1的过程会失败，因为state的值当前为1，说明已经有人加锁了！
            接着线程2会看一下，是不是自己之前加的锁啊？当然不是了，“加锁线程”这个变量明确记录了是线程1占用了这个锁，所以线程2此时就是加锁失败。
            接着，线程2会将自己放入AQS中的一个等待队列，因为自己尝试加锁失败了，此时就要将自己放入队列中来等待，等待线程1释放锁之后，自己就可以重新尝试加锁了
            AQS内部还有一个等待队列，专门放那些加锁失败的线程！
            接着，线程1在执行完自己的业务逻辑代码之后，就会释放锁！他释放锁的过程非常的简单，就是将AQS内的state变量的值递减1，如果state值为0，则彻底释放锁，会将“加锁线程”变量也设置为null！
            接下来，会从等待队列的队头唤醒线程2重新尝试加锁。
            线程2现在就重新尝试加锁，这时还是用CAS操作将state从0变为1，此时就会成功，成功之后代表加锁成功，就会将state设置为1。
            还要把“加锁线程”设置为线程2自己，同时线程2自己就从等待队列中出队了。


2：HashMap 的工作原理？
    HashMap 底层是 hash 数组和单向链表实现，数组中的每个元素都是链表，由 Node 内部类（实现 Map.Entry接口）实现，HashMap 通过 put & get 方法存储和获取。
    存储对象时，将 K/V 键值传给 put() 方法：
    ①、调用 hash(K) 方法计算 K 的 hash 值，然后结合数组长度，计算得数组下标；
    ②、调整数组大小（当容器中的元素个数大于 capacity * loadfactor 时，容器会进行扩容resize 为 2n）；
    ③、i.如果 K 的 hash 值在 HashMap 中不存在，则执行插入，若存在，则发生碰撞；
    ii.如果 K 的 hash 值在 HashMap 中存在，且它们两者 equals 返回 true，则更新键值对；
    iii. 如果 K 的 hash 值在 HashMap 中存在，且它们两者 equals 返回 false，则插入链表的尾部（尾插法）或者红黑树中（树的添加方式）。（JDK 1.7 之前使用头插法、JDK 1.8 使用尾插法）（注意：当碰撞导致链表大于 TREEIFY_THRESHOLD = 8 时，就把链表转换成红黑树）
    获取对象时，将 K 传给 get() 方法
    ：①、调用 hash(K) 方法（计算 K 的 hash 值）从而获取该键值所在链表的数组下标；
    ②、顺序遍历链表，equals()方法查找相同 Node 链表中 K 值对应的 V 值。
    hashCode 是定位的，存储位置；equals是定性的，比较两者是否相等。
4.你知道 hash 的实现吗？为什么要这样实现？
    JDK 1.8 中，是通过 hashCode() 的高 16 位异或低 16 位实现的：
    (h = k.hashCode()) ^ (h >>> 16)，主要是从速度，功效和质量来考虑的，减少系统的开销，也不会造成因为高位没有参与下标的计算，从而引起的碰撞。


7.HashMap中put方法的过程？
    答：“调用哈希函数获取Key对应的hash值，再计算其数组下标；
    如果没有出现哈希冲突，则直接放入数组；如果出现哈希冲突，则以链表的方式放在链表后面；
    如果链表长度超过阀值( TREEIFY THRESHOLD==8)，就把链表转成红黑树，链表长度低于6，就把红黑树转回链表;
    如果结点的key已经存在，则替换其value即可；
    如果集合中的键值对大于12，调用resize方法进行数组扩容。”

8.接收窗口 rwnd 和发送窗口 cwnd
    此处涉及到二个窗口：
    接收窗口receiver window(即rwnd)
        是接收方根据自己的承受能力设置的接收缓存值大小，反映了接收方的接收能力，来做流量控制。
    拥塞窗口congestion window(即cwnd)
        是发送方根据网络拥塞程度设置的网络窗口值，发送窗口=min(rwnd，cwnd)即是接收窗口和拥塞窗口的最小值，来做拥塞控制。

9.流量控制和拥塞控制的区别
    1、拥塞控制是防止过多的数据注入到网络中，可以使网络中的路由器或链路不致过载，是一个全局性的过程。
    2、流量控制是点对点通信量的控制，是一个端到端的问题，主要就是抑制发送端发送数据的速率，以便接收端来得及接收

9.流量控制
    流量控制目的：让发送方根据网络状况动态的调整发送频率，好让接收方来得及接收。
    方式：采用滑动窗口（滑动窗口就是未接受确认应答的数据段）
    假如对方给我确认应答序号，可接受窗口大小为0，怎么办？
        使用一个持续计数器，此时我方会启动一个定时器，定时发送一个试探报文给对方，试探报文没有数据的，当对方回复我方窗口大小不为0时继续传输数据；如果为0，重新启动计时器。
    避免死锁：
        持续定时器（persist timer），他的作用是，发送方只要接收到了0窗口通告，就开启该定时器，周期性的向接收方发送1字节的0窗口探测报文。
    死锁问题
        1、B向A发送了零窗口的报文段后不久，B的接收缓存又有了一些存储空间。于是B向A发送了rwnd = 400的报文段，然而这个报文段在传送过程中丢失了。
        2、A一直等待收到B发送的非零窗口的通知，而B也一直等待A发送的数据，如果没有其他措施，这种相互等待的死锁局面将一直持续下去。
    死锁解决方法：
        1.TCP为每一个连接设有一个持续计时器。
        2.只要TCP连接的一方收到对方的零窗口通知，就启动持续计时器。
        3.若持续计时器设置的时间到期，就发送一个零窗口探测报文段（仅携带1字节的数据），而对方就在确认这个探测报文时给出了现在的窗口值。
        4.如果窗口值仍然是零，那么收到这个报文段的一方就重新设置持续计时器。
        5.如果窗口不是零，那么死锁的僵局就可以打破了。

10.拥塞控制
    拥塞控制的目：防止过多的数据注入到网络中，网络堵塞使得包一直到不了接收端。
    拥塞控制的方法
        1.慢开始
        2.拥塞避免
        3.快重传
        4.快恢复
    TCP的拥塞控制采用的是窗口机制，通过调节窗口的大小实现对数据发送速率的调整
    TCP的发送端维持一个称为拥塞窗口cwnd的变量，单位为字节。
        为了防止拥塞窗口cwnd增长过大引起网络拥塞，还需要设置一个慢开始门限ssthresh状态变量
        1.当cwnd < ssthresh时，使用上述的慢开始算法
        2.当cwnd > ssthresh时， 停止使用慢开始算法而改用拥塞避免算法
        3.当cwnd = ssthresh时 ， 即可以使用慢开始算法，也可以使用拥塞避免算法。
        什么是RTT
        1、一个传输轮次所经历的时间其实就是往返时间RTT（RTT并非恒定的数值）
        例如，拥塞窗口cwnd的大小是4个报文段，那么这时的往返时间RTT就是发送方连续发送4个报文段，并收到这4个报文段的确认，总共经历的时间。
        2、使用“传输轮次”是更加强调：把拥塞窗口所允许发生的报文段都连续发送出去，并收到了对已发送的最后一个字节的确认
        3、在TCP的实际运行中，发送方只要收到一个对新报文段的确认，其拥塞窗口cwnd就立即加1，并可以立即发送新的报文段，而不需要等这个轮次中所有的确认都收到后再发送新的报文段。


9.浅析spark中的三种join方式：
    1、broadcast join：
        适用于小表与大表的join，其中小表小于10M，本质上是去用空间换时间，
        也就是将小表发送到每个大表对应的节点上，然后依次去遍历大表中的每个关联键去和对应小表中的关联键去匹配，找到相等的进行关联，并不会走shuffle过程，因此最终的文件数取决于block的个数，每个bolck是一个逻辑上的概念，
        生成的规则是在建表并落HDFS文件的时候按256M去拆分，得到最终的文件数。
    2、shuffle hash join
        适合于没有特别小的两个表进行关联的时候，默认设置的shuffle partition的个数
        是200，也就是分了200个区，然后两张表的key值分别去基于200做hash取余然后散步在每个区域中了，这样的思想先把相近的合并在一个区内，再在每个分区内去做比较key值的等值比较，就避免了大范围的遍历比较，节省了时间和内存。
    3、sort merge join
        这种适用于关联的两张表都特别大时，使用上述的两种方法加载到内存的时候对于内存的压力都非常大时，因此在2方法的基础上，hash取余之后还要分别对两张表的key值进行排序，这样去做等值比较的时候就不需要将某一方的全部数据都加载到内存进行计算了，只需要取一部分就能知道是否有相等的（比如按升序排列，某个值明显比它大了，后面肯定就不会有相等的，就不用继续比较了，节省了时间和内存），也就是在进行等值比较的时候即用即丢的。
        这个方法在前面进行排序的时候可能会消耗点时间，但相对于后面的时间来说，总体是大大节省了时间。

为啥分为action和transform：
    这个问题本质上讲很复杂，这是spark设计上必须要这么做，不然spark就无法干掉mapreduce。
    1. 首先我们需要理解这惰性计算
        spark将对RDD的操作分为transformation和action，action一共只有五个操作：save、count、collect、lookup、reduce。
        当然像distinct这种操作我们认为它是复合操作，中间过程包括reduce。
        只有当action执行，才会整理计算逻辑，创建job，并生成DAG，然后在DAG根据shuffle操作（宽依赖、窄依赖）更细粒度的拆分stage，启动相应数量的task计算中间结果，stage by stage的计算，就这样支撑了整个spark计算的框架。
    2. 这种惰性计算是最优做法
        所谓的惰性计算，其实只是表象，本质上都是并行计算（流水线计算），比如map1 -> filter -> map2 -> collect。a1做完map1，进入filter，此时a2进入map1...其实并不是惰性计算。
    3. 如果讲到spark设计，本质上这种设计很棒，没什么问题。同时也在考虑一个问题内存管理以及高可用。对于mapreduce来讲，在计算层面不会考虑容灾问题，因为底层hdfs的多副本会保证数据安全，但是spark就没那么幸运了，再设计时不会落盘（尽量不要），那怎么解决容灾问题？RDD采用的是dependency解决，在这种情况下拆分成stage就很好，因为这种粒度大一些，一旦数据丢失只会重启stage。如果不是惰性计算，那就倒霉了，每一个操作都将是一个RDD，对于spark来讲这种粒度太细了，管理起来不容易。

Java中static关键字：
    static 关键字可以用来修饰：属性、方法、内部类、代码块；
    static 修饰的资源属于类级别，是全体对象实例共享的资源；
    使用 static 修饰的属性，静态属性是在类的加载期间初始化的，使用类名.属性访问
　　 把一个变量声明为静态变量通常基于以下三个目的：
    使用静态变量的目的：
        作为共享变量使用
        减少对象的创建
        保留唯一副本
5、static修饰的数据是共享数据，对象中的存储的是特有的数据。

成员变量和静态变量的区别：
    1、生命周期的不同：
        成员变量随着对象的创建而存在随着对象的回收而释放。
        静态变量随着类的加载而存在随着类的消失而消失。

    2、调用方式不同：
        成员变量只能被对象调用。
        静态变量可以被对象调用，也可以用类名调用。（推荐用类名调用）

    3、别名不同：
        成员变量也称为实例变量。
        静态变量称为类变量。

    4、数据存储位置不同：
        成员变量数据存储在堆内存的对象中，所以也叫对象的特有数据。
        静态变量数据存储在方法区（共享数据区）的静态区，所以也叫对象的共享数据。

string为什么用final修饰：
    什么是不可变对象？
        如果一个对象，在它创建完成之后，不能再改变它的状态，那么这个对象就是不可变的。
        不能改变状态的意思是，不能改变对象内的成员变量，包括基本数据类型的值不能改变，引用类型的变量不能指向其他的对象，引用类型指向的对象的状态也不能改变。
    string底层原理
        String底层是由char数组构成，我们创建一个字符串对象的时候，其实是将字符串保存在char数组中
        因为数组是引用对象，为了防止数组可变，JDK加了final修饰
        但是加了final修饰的数组只是代表了引用不可变，不代表数组内容不可变
        因此JDK为了真正防止不可变，又加了private修饰符。
    不可变的好处：
        1、多线程下的安全性
            因为String是不可变的，因此多线程操作下，它是安全的
        2、类加载中体现的安全性
            类加载器要用到字符串，不可变提供了安全性，以便正确的类被加载，例如你想加载java.sql.Connection类，而这个值被改成了xxx.Connection，那么会对你的数据库造成不可知的破坏。
        3、使用常量池可以节省空间
            像下面这样字符串one和two都用字面量"something"赋值。它们其实都指向同一个内存地址
            1String one = "someString";
            2String two = "someString";
            这样在大量使用字符串的情况下，可以节省内存空间，提高效率。
            但之所以能实现这个特性，String的不可变性是最基本的一个必要条件。
            要是内存里字符串内容能改来改去，这么做就完全没有意义了。


Java反射机制：
    ​ Java 的反射机制是指在运行状态中，对于任意一个类都能够知道这个类所有的属性和方法；
     并且对于任意一个对象，都能够调用它的任意一个方法；
     这种动态获取信息以及动态调用对象方法的功能成为Java语言的反射机制

获取 Class 类对象三种方式：
    使用 Class.forName 静态方法
    使用类的.class 方法
    使用实例对象的 getClass() 方法

Java面向对象的三大特点：
    一、封装
        1、封装的概念
        　　封装性是面向对象编程的核心思想
        　　指的就是将描述某种实体的数据和基于这些数的操作集合到一起，形成一个封装体
        　　封装的思想保证了类内部数据结构的完整性，使用户无法轻易直接操作类的内部数据，这样降低了对内部数据的影响，提高了程序的安全性和可维护性。
        2、封装的好处
        　　只能通过规定方法访问数据
        　　隐藏类数实现细节
        　　方便修改实现
        　　方便加入控制语句　　
        3、封装的使用
        　　1）、修改属性的可见性   ——> 设为private
        　　2）、创建共有的 getter / setter方法 ——> 用于属性的读写
        　　3）、在getter / setter方法中加入属性控制语句 ——> 对属性值的合法性进行判断
   二、继承的概念和特点
   概念：
    　　继承是Java面向对象编程技术的一块基石，因为它允许创建分等级层次的类。
    　　继承就是子类继承父类的特征和行为，使得子类对象（实例）具有父类的实例域和方法，或类从父 类继承方法，使得子类具有父类相同的行为。
   三、多态
   　　将父类对象应用于子类对象的特征就是面向对象编程中的多态性的体现
   　　多态指的就是在应用程序中出现的 “重名” 现象。多态性允许以统一的风格编写程序，以处理种类繁多的已存在的类及其相关类。这样既降低了维护难度，又节省了时间

多线程的执行流程：
    1、 当一个任务通过submit或者execute方法提交到线程池的时候，如果当前池中线程数（包括闲置线程）小于coolPoolSize，则创建一个线程执行该任务。
    2、如果当前线程池中线程数已经达到coolPoolSize，则将任务放入等待队列。
    3、如果任务不能入队，说明等待队列已满，若当前池中线程数小于maximumPoolSize，则创建一个临时线程（非核心线程）执行该任务。
    4、如果当前池中线程数已经等于maximumPoolSize，此时无法执行该任务，根据拒绝执行策略处理。
    注意：当池中线程数大于coolPoolSize，超过keepAliveTime时间的闲置线程会被回收掉。回收的是非核心线程，核心线程一般是不会回收的。如果设置allowCoreThreadTimeOut(true)，则核心线程在闲置keepAliveTime时间后也会被回收。
    任务队列是一个阻塞队列，线程执行完任务后会去队列取任务来执行，如果队列为空，线程就会阻塞，直到取到任务。

四种线程池：
    Java通过Executors提供四种线程池，分别为：
    1、newSingleThreadExecutor
        创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。
        底层：FinalizableDelegatedExecutorService包装的ThreadPoolExecutor实例，corePoolSize为1；maximumPoolSize为1；keepAliveTime为0L；unit为：TimeUnit.MILLISECONDS；
        workQueue为：new LinkedBlockingQueue<Runnable>() 无界阻塞队列
        通俗：创建只有一个线程的线程池，且线程的存活时间是无限的；当该线程正繁忙时，对于新任务会进入阻塞队列中(无界的阻塞队列)
        适用：一个任务一个任务执行的场景
    2、newFixedThreadPool
        创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。
        底层：返回ThreadPoolExecutor实例，接收参数为所设定线程数量nThread，corePoolSize为nThread，maximumPoolSize为nThread；keepAliveTime为0L(不限时)；unit为：TimeUnit.MILLISECONDS；
        WorkQueue为：new LinkedBlockingQueue<Runnable>() 无界阻塞队列
        通俗：创建可容纳固定数量线程的池子，每隔线程的存活时间是无限的，当池子满了就不在添加线程了；如果池中的所有线程均在繁忙状态，对于新任务会进入阻塞队列中(无界的阻塞队列)
        适用：执行长期的任务，性能好很多
        缺点：无界队列，容易造成任务阻塞
    3、newScheduledThreadPool
        创建一个可定期或者延时执行任务的定长线程池，支持定时及周期性任务执行。
        底层：创建ScheduledThreadPoolExecutor实例，corePoolSize为传递来的参数，maximumPoolSize为Integer.MAX_VALUE；keepAliveTime为0；unit为：TimeUnit.NANOSECONDS；
        workQueue为：new DelayedWorkQueue() 一个按超时时间升序排序的队列
        通俗：创建一个固定大小的线程池，线程池内线程存活时间无限制，线程池可以支持定时及周期性任务执行，如果所有线程均处于繁忙状态，对于新任务会进入DelayedWorkQueue队列中，这是一种按照超时时间排序的队列结构
        适用：周期性执行任务的场景
    4、newCachedThreadPool
        创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。
        底层：返回ThreadPoolExecutor实例，corePoolSize为0；maximumPoolSize为Integer.MAX_VALUE；keepAliveTime为60L；unit为TimeUnit.SECONDS；
        workQueue为SynchronousQueue(同步队列)
        通俗：当有新任务到来，则插入到SynchronousQueue中，由于SynchronousQueue是同步队列，因此会在池中寻找可用线程来执行，若有可以线程则执行，若没有可用线程则创建一个线程来执行该任务；若池中线程空闲时间超过指定大小，则该线程会被销毁。
        适用：执行很多短期异步的小程序或者负载较轻的服务器
        1.它是一个可以无限扩大的线程池；
        2.它比较适合处理执行时间比较小的任务；corePoolSize为0，maximumPoolSize为无限大，意味着线程数量可以无限大；
        3.keepAliveTime为60S，意味着线程空闲时间超过60S就会被杀死；
        4.采用SynchronousQueue装等待的任务，这

线程池的7个参数：
    一、corePoolSize 线程池核心线程大小
        线程池中会维护一个最小的线程数量，即使这些线程处理空闲状态，他们也不会 被销毁，除非设置了allowCoreThreadTimeOut。这里的最小线程数量即是corePoolSize。
    二、maximumPoolSize 线程池最大线程数量
        一个任务被提交到线程池后，首先会缓存到工作队列（后面会介绍）中，如果工作队列满了，则会创建一个新线程，然后从工作队列中的取出一个任务交由新线程来处理，而将刚提交的任务放入工作队列。线程池不会无限制的去创建新线程，它会有一个最大线程数量的限制，这个数量即由maximunPoolSize来指定。
    三、keepAliveTime 空闲线程存活时间
        一个线程如果处于空闲状态，并且当前的线程数量大于corePoolSize，那么在指定时间后，这个空闲线程会被销毁，这里的指定时间由keepAliveTime来设定
    四、unit 空间线程存活时间单位
        keepAliveTime的计量单位
    五、workQueue 工作队列
        新任务被提交后，会先进入到此工作队列中，任务调度时再从队列中取出任务。jdk中提供了四种工作队列：
        ①ArrayBlockingQueue
            基于数组的有界阻塞队列，按FIFO排序。新任务进来后，会放到该队列的队尾，有界的数组可以防止资源耗尽问题。当线程池中线程数量达到corePoolSize后，再有新任务进来，则会将任务放入该队列的队尾，等待被调度。如果队列已经是满的，则创建一个新线程，如果线程数量已经达到maxPoolSize，则会执行拒绝策略。
        ②LinkedBlockingQueue
            基于链表的无界阻塞队列（其实最大容量为Interger.MAX），按照FIFO排序。由于该队列的近似无界性，当线程池中线程数量达到corePoolSize后，再有新任务进来，会一直存入该队列，而不会去创建新线程直到maxPoolSize，因此使用该工作队列时，参数maxPoolSize其实是不起作用的。
        ③SynchronousQueue
            一个不缓存任务的阻塞队列，生产者放入一个任务必须等到消费者取出这个任务。也就是说新任务进来时，不会缓存，而是直接被调度执行该任务，如果没有可用线程，则创建新线程，如果线程数量达到maxPoolSize，则执行拒绝策略。
            如果以洗盘子的比喻为例，那么这就相当于没有盘架，而是将洗好的盘子直接放入下一个空闲的烘干机中。这种实现队列的方式看似很奇怪，但由于可以直接交付工作，从而降低了将数据从生产者移动到消费者的延迟。
        ④PriorityBlockingQueue
            具有优先级的无界阻塞队列，优先级通过参数Comparator实现。
    六、threadFactory 线程工厂
        创建一个新线程时使用的工厂，可以用来设定线程名、是否为daemon线程等等
    七、handler 拒绝策略
        当工作队列中的任务已到达最大限制，并且线程池中的线程数量也达到最大限制，这时如果有新任务提交进来，该如何处理呢。这里的拒绝策略，就是解决这个问题的，jdk中提供了4中拒绝策略：
        ①CallerRunsPolicy
            该策略下，在调用者线程中直接执行被拒绝任务的run方法，除非线程池已经shutdown，则直接抛弃任务。
        ②AbortPolicy
            该策略下，直接丢弃任务，并抛出RejectedExecutionException异常。
        ③DiscardPolicy
            该策略下，直接丢弃任务，什么都不做。
        ④DiscardOldestPolicy
            该策略下，抛弃进入队列最早的那个任务，然后尝试把这次拒绝的任务放入队列
OLAP和OLTP的区别：
    联机事务处理OLTP（on-line transaction processing）
    联机分析处理OLAP（On-Line Analytical Processing）。
    1.
        OLAP是数据仓库系统的主要应用，支持复杂的分析操作，侧重决策支持，并且提供直观易懂的查询结果。
        OLAP（On-Line Analytical Processing）联机分析处理，也称为面向交易的处理过程，其基本特征是前台接收的用户数据可以立即传送到计算中心进行处理，并在很短的时间内给出处理结果，是对用户操作快速响应的方式之一。
        应用在数据仓库，使用对象是决策者。
        OLAP系统强调的是数据分析，响应速度要求没那么高。
    2.
        OLTP是传统的关系型数据库的主要应用，主要是基本的、日常的事务处理，例如银行交易。
        OLTP（On-Line Transaction Processing）联机事务处理，它使分析人员能够迅速、一致、交互地从各个方面观察信息，以达到深入理解数据的目的。它具有FASMI(Fast Analysis of Shared Multidimensional Information)，即共享多维信息的快速分析的特征。
        主要应用是传统关系型数据库。OLTP系统强调的是内存效率，实时性比较高。
flink savepoint checkpoint区别
    功能的原理是一样的, 只是目的不同, 一个是自动的备份, 另一个是手动的保持状态, 用做版本升级/服务重启等
flink checkpoint原理
    意义: 被checkpoint保存的每个subtask的状态只有raw state和managed state两种。
    raw state是用户自己进行序列化，而managed state是在operator生命周期初始化时就被注册到backend storage对象中了，在进行checkpoint时，会直接拿到注册的state进行保存（中间会调用回调函数，在UDF中对state进行赋值）。
    所以checkpoint的state不是很大的数据。
    如何做到checkpoint:
        理想情况下, checkpoint是最好可以把输入流停掉, 然后保持这一时刻 所有分区的快照
        但是这样在工程上不合理, 所以用了Chandy-Lamport算法 做一个分布式的快照
        在checkpoint触发时刻，Job Manager会往所有Source的流中放入一个barrier。barrier包含当前checkpoint的ID
        当barrier经过一个subtask时，即表示当前这个subtask处于checkpoint触发的“时刻”，他就会立即将barrier发往下游，并执行checkpoint方法将当前的state存入backend storage

Flink多个stream合并和拆分
    2.Split 和 Select （使用split切分过的流是不能被二次切分的）
        DataStream --> SplitStream : 根据特征把一个DataSteam 拆分成两个或者多个DataStream.
        SplitStream --> DataStream:从一个SplitStream中获取一个或者多个DataStream。
    3.Connect 和 CoMap / CoFlatMap
         DataStream,DataStream --> ConnectedStream:连接两个保持他们类型的数据流，两个数据流被Connect之后，只是被放在了一个同一个流中，内部依然保持着各自的数据和形式，不发生变化，两个流相互独立。
         ConnectedStream --> DataStream：作用与 ConnectedStream上，功能与map和Flatmap一样，对 ConnectedStream中的每一个Stream分别进行map和flatmap处理。
    4、union、connect
         DataStream --> DataStream:对两个或者两个以上的DataStream进行union操作，产生一个包含所有DataStream元素的新DataStream
         使用：DataStream dataStream = pcDataStream.union(appDataStream).union(mDataStream);
         注意：Connect 与 Union区别：
            1、Union之前两个流的类型必须是一样的，Conect可以不一样，并且Connect之后进行coMap中调整为一样的。
            2、Connect只能操作两个流，Union可以操作多个。

RDD、dataframe、dataset的区别


hive-sql,spark-sql的原理：
    然后就是问hive sql的执行原理, hive/spark sql/ mysql/ calcite 其实都有着共性
    就是分为了 sql语法词法解析为抽象语法树AST -> 逻辑计划生成和优化 -> 物理计划生成和优化
    比如sql语法词法解析为抽象语法树AST
    calcite用了javaCC catalyst用了anltr4 ,巴拉巴拉 然后是逻辑计划的生成, 然后是逻辑计划的优化
    逻辑计划生成和优化：属于基于关系型代数来对sql语法进行同义替换的过程
    物理疾患生成和优化：然后到了具体的物理执行计划的时候 才有各自的区别, 比如hive是把相关的逻辑计划翻译成map reduce等操作, spark sql是把他翻译成rdd直接的操作 其实整体来看 殊途同归

hive laterval view udtf(col) tableAlias as columnAlias

Kafka多副本之间数据如何同步？
    多个副本之间数据是如何同步的？其实任何一个 Partition，只有 Leader 是对外提供读写服务的。
    也就是说，如果有一个客户端往一个 Partition 写入数据，此时一般就是写入这个 Partition 的 Leader 副本。
    然后 Leader 副本接收到数据之后，Follower 副本会不停的给它发送请求尝试去拉取最新的数据，拉取到自己本地后，写入磁盘中。
Kafka如何保证可靠性和一致性：
    1.Kafka 的分区多副本架构是 Kafka 可靠性保证的核心，把消息写入多个副本可以使 Kafka 在发生崩溃时仍能保证消息的持久性。
    2.Producer 往 Broker 发送消息，在 Producer 里面提供了消息确认机制，也就是说我们可以通过配置来决定消息发送到对应分区的几个副本才算消息发送成功
        1.acks = 0：意味着如果生产者能够通过网络把消息发送出去，那么就认为消息已成功写入 Kafka。
        2.acks = 1：意思就是说只要 Partition Leader 接收到消息而且写入本地磁盘了，就认为成功了，不管它其他的 Follower 有没有同步过去这条消息了。
                    意味若 Leader 在收到消息并把它写入到分区数据文件（不一定同步到磁盘上）时会返回确认或错误响应。
                    这种设置其实是 Kafka 默认的设置
                    万一 Partition Leader 刚刚接收到消息，Follower 还没来得及同步过去，结果 Leader 所在的 Broker 宕机了，此时也会导致这条消息丢失，因为人家客户端已经认为发送成功了。
        3.acks = all：这个意思就是说，Partition Leader 接收到消息之后，还必须要求 ISR 列表里跟 Leader 保持同步的那些 Follower 都要把消息同步过去，才能认为这条消息是写入成功了。
                    意味着 Leader 在返回确认或错误响应之前，会等待所有同步副本都收到悄息。
    3.ack应答机制
            由于数据的重要程度是不一样的，有些可以少量允许丢失，希望快一点处理；有些不允许，希望稳妥一点处理，所以没必要所有的数据处理的时候都等ISR中的follower全部接收成功。因此kafka处理数据时为了更加灵活，给用户提供了三种可靠性级别，用户可以通过调节acks参数来选择合适的可靠性和延迟。
            acks的参数分别可以配置为：0，1，-1。
            它们的作用分别是：
                配置为0：producer不等待broker的ack，这一操作提供了一个最低的延迟，broker一接收到还没有写入磁盘就已经返回，当broker故障时有可能丢失数据；
                配置为1：producer等待broker的ack，partition的leader写入磁盘成功后返回ack，但是如果在follower同步成功之前leader故障，那么将会丢失数据；
                配置为-1：producer等待broker的ack，partition的leader和follower全部写入磁盘成功后才返回ack。但是如果在follower同步完成后，broker发送ack之前，leader发生故障，此时会选举新的leader，但是新的leader已经有了数据，但是由于没有之前的ack，producer会再次发送数据，那么就会造成数据重复。

    3.leader选举机制
        ISR（in-sync replicas）列表。每个分区的 leader 会维护一个 ISR 列表，ISR 列表里面就是 follower 副本的 Borker 编号
        只有跟得上 Leader 的 follower 副本才能加入到 ISR 里面
        通过replica.lag.time.max.ms 参数进行配置。
        所以当 Leader 挂掉了，而且 unclean.leader.election.enable=false 的情况下
        Kafka 会从 ISR 列表中选择第一个 follower 作为新的 Leader
        因为这个分区拥有最新的已经 committed 的消息。通过这个可以保证已经 committed 的消息的数据可靠性。
Kafka的数据一致性：
    数据一致性主要是说不论是老的 Leader 还是新选举的 Leader，Consumer 都能读到一样的数据。那么 Kafka 是如何实现的呢？
    只有 High Water Mark 以上的消息才支持 Consumer 读取，类似于木桶原理。而 High Water Mark 取决于 ISR 列表里面偏移量最小的分区，对应于上图的副本2，这个很类似于木桶原理。
    这样做的原因是还没有被足够多副本复制的消息被认为是“不安全”的，如果 Leader 发生崩溃，另一个副本成为新 Leader，那么这些消息很可能丢失了。
    如果我们允许消费者读取这些消息，可能就会破坏一致性。
    试想，一个消费者从当前 Leader（副本0） 读取并处理了 Message4，这个时候 Leader 挂掉了，选举了副本1为新的 Leader，这时候另一个消费者再去从新的 Leader 读取消息，发现这个消息其实并不存在，这就导致了数据不一致性问题。
    当然，引入了 High Water Mark 机制，会导致 Broker 间的消息复制因为某些原因变慢，那么消息到达消费者的时间也会随之变长（因为我们会先等待消息复制完毕。
    延迟时间可以通过参数 replica.lag.time.max.ms 参数配置，它指定了副本在复制消息时可被允许的最大延迟时间。

kafka消费者消费方式:
    先说结论：消费者采用pull（拉）模式从broker中读取数据。
    为什么不采用push（推，填鸭式教学）的模式给消费者数据呢？首先回想下咱们上学学习不就是各种填鸭式教学吗？不管你三七二十一，就是按照教学进度给你灌输知识，能不能接受是你的事，并美其名曰：优胜略汰！
    这种push方式在kafka架构里显然是不合理的，比如一个broker有多个消费者，它们的消费速率不同，一昧的push只会给消费者带来拒绝服务以及网络拥塞等风险。而kafka显然不可能去放弃速率低的消费者，因此kafka采用了pull的模式，可以根据消费者的消费能力以适当的速率消费broker里的消息。
    当然让消费者去pull数据自然也是有缺点的。同样联想上学的场景，如果把学习主动权全部交给学生，那有些学生想学的东西老师那里没有怎么办？那他不就陷入了一辈子就在那不断求索，然而别的也啥都学的这个死循环的状态了。kafka也是这样，采用pull模式后，如果kafka没有数据，消费者可能会陷入循环中，一直返回空数据。
    为了解决这个问题，Kafka消费者在消费数据时会传入一个时长参数timeout，如果当前没有数据可供消费，消费者会等待一段时间之后再返回，这段时长即为timeout。

Kafka分区：
    1、为什么要进行分区
        在了解分区策略之前需要先了解为什么要分区，可以从两方面来解释这个问题：
        方便在集群中扩展，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据；
        可以提高并发，分区后以Partition为单位读写。
    2、

kafka分区分配策略:
    1、Range分配策略
        Range分配策略是面向每个主题的，首先会对同一个主题里面的分区按照序号进行排序，并把消费者线程按照字母顺序进行排序。然后用分区数除以消费者线程数量来判断每个消费者线程消费几个分区。如果除不尽，那么前面几个消费者线程将会多消费一个分区。
    2、RoundRobin分配策略
        RoundRobin策略的原理是将消费组内所有消费者以及消费者所订阅的所有topic的partition按照字典序排序，然后通过轮询算法逐个将分区以此分配给每个消费者。
    3、Sticky分配策略
    最后介绍一下Sticky分配策略，这种分配策略是在kafka的0.11.X版本才开始引入的，是目前最复杂也是最优秀的分配策略。
    Sticky分配策略的原理比较复杂，它的设计主要实现了两个目的：
        分区的分配要尽可能的均匀；
        分区的分配尽可能的与上次分配的保持相同。
    这是因为发生分区重分配后，对于同一个分区而言有可能之前的消费者和新指派的消费者不是同一个，对于之前消费者进行到一半的处理还要在新指派的消费者中再次处理一遍，这时就会浪费系统资源。而使用Sticky策略就可以让分配策略具备一定的“粘性”，尽可能地让前后两次分配相同，进而可以减少系统资源的损耗以及其它异常情况的发生。
flink消费kafka:
    flink sql 通过ddl读取和写入kafka怎么设置并行度呢？
    flink sql 通过ddl写入kafka怎么自定义分区呢？
    1.首先第一个问题我们可以为咱们的程序设置默认的并发度，用代码或者命令行参数，配置文件都可以。
    2.第二个问题可以将 connector.sink-partitioner设置为 custom, 然后设置 connector.sink-partitioner-class




Flink反压：
    Flink 内部是基于 producer-consumer 模型来进行消息传递的，Flink的反压设计也是基于这个模型。
    Flink 使用了高效有界的分布式阻塞队列，就像 Java 通用的阻塞队列（BlockingQueue）一样。
    下游消费者消费变慢，上游就会受到阻塞。


上下文切换的开销：
    上下文切换的开销包括直接开销和间接开销。
直接开销有如下几点：（时间开销）
    操作系统保存回复上下文所需的开销
    线程调度器调度线程的开销

间接开销有如下几点：（缓存开销）
    处理器高速缓存重新加载的开销
    上下文切换可能导致整个一级高速缓存中的内容被冲刷，即被写入到下一级高速缓存或主存


Spark Streaming在内部的处理机制 sparkstreaming
    接收实时流的数据，并根据一定的时间间隔拆分成一批批的数据，然后通过Spark Engine处理这些批数据
    最终得到处理后的一批批结果数据。对应的批数据，在Spark内核对应一个RDD实例
    因此，对应流数据的DStream可以看成是一组RDDs，即RDD的一个序列。
    通俗点理解的话，在流数据分成一批一批后，通过一个先进先出的队列，然后 Spark Engine从该队列中依次取出一个个批数据，把批数据封装成一个RDD，然后进行处理
    这是一个典型的生产者消费者模型，对应的就有生产者消费者模型的问题，即如何协调生产速率和消费速率

group by实现原理:
    mysql中group by实现方式有三种，松散索引，紧凑索引，临时文件（文件排序）
    1.松散索引
        对group by操作优化的原理就是让mysql利用索引，而避免进行建立临时表，进而进行文件排序（group by的第三种实现方式）
        对于group by引用的多个字段，需满足于所建立索引的最左前缀索引，否则进行group by操作时，无法利用索引。
        在利用索引时，group by可根据索引，即可对数据分组，此时完全不用去访问表的数据值（索引健对应的数据）
    2.紧凑索引
        当group by引用的字段无法构成所建索引的最左前缀索引时，也就是说group by不能利用索引时。
        如何where语句（如果有的话）弥补了这种差距，比如：group by
        引用的字段为（c2，c3），而索引为（c1，c2，c3）。此时如果where语句限定了c1=a（某一个值），那么此时mysql的执行过程为先根据where语句进行一次选择，
        对选出来的结果集，可以利用索引。
        这种方式，从整体上来说，group by并没有利用索引，但是从过程来说，在选出的结果中利用了索引，这种方式就是紧凑索引。
        mysql的执行计划为using where,use index。而松散索引的执行计划为using index for group by
    3.临时文件
        如果mysql如论如何都不能利用索引时，此时mysql将读取所有的数据建立临时表，对文件进行排序，完成分组操作

乐观锁和悲观锁：
    何谓悲观锁与乐观锁
        乐观锁对应于生活中乐观的人总是想着事情往好的方向发展，悲观锁对应于生活中悲观的人总是想着事情往坏的方向发展。这两种人各有优缺点，不能不以场景而定说一种人好于另外一种人。
    悲观锁
        总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程）。
        传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。
        Java中synchronized和ReentrantLock等独占锁就是悲观锁思想的实现。
    乐观锁
        总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现。
        乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于write_condition机制，其实都是提供的乐观锁。
        在Java中java.util.concurrent.atomic包下面的原子变量类就是使用了乐观锁的一种实现方式CAS实现的。
    使用场景
        两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。
        但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行retry，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。
线程池的实现原理
    其实java线程池的实现原理很简单，说白了就是一个线程集合workerSet和一个阻塞队列workQueue。
    当用户向线程池提交一个任务(也就是线程)时，线程池会先将任务放入workQueue中。
    workerSet会不断的从workQueue中获取线程然后执行。
    当workQueue中没有任务的时候，worker就会阻塞，直到队列中有任务了就取出来继续执行。
    线程池7大参数:
        corePoolSize: 核心线程数量，规定线程池有几个线程(worker)在运行。
        maximumPoolSize: 当workQueue满了,不能添加任务的时候，这个参数才会生效。规定线程池最多只能有多少个线程(worker)在执行。
        keepAliveTime: 超出corePoolSize大小的那些线程的生存时间,这些线程如果长时间没有执行任务并且超过了keepAliveTime设定的时间，就会消亡。
        unit: 生存时间对于的单位
        workQueue: 存放任务的队列
        threadFactory: 创建线程的工厂
        handler: 当workQueue已经满了，并且线程池线程数已经达到maximumPoolSize，将执行拒绝策略。
HDFS为什么副本是3:
    HDFS采用一种称为机架感知的策略来改进数据的可靠性、可用性和网络带宽的利用率
    在大多数情况下，HDFS的副本系数是3，HDFS的存放策略是一个副本存放在本地机架节点上，另一个副本存放在同一机架的另一个节点上，第三个副本存放在在不同机架的节点上。这种策略减少了机架间的数据传输，提高了写操作的效率。机架错误的概率远比节点错误的概率小，所以这种策略不会对数据的可靠性和可用性造成影响。与此同时，因为数据只存在两个机架上，这种策略减少了读数据时需要的网络传输带宽。
    在这种策略下，副本并不是均匀地分布在机架上。这种策略在不损坏可靠性和读取性能的情况下，改善了写的性能

TCP粘包:
    TCP粘包是指发送方发送的若干包数据到接收方接收时粘成一包，从接收缓冲区看，后一包数据的头紧接着前一包数据的尾。
    TCP是端到端传输的，同时TCP连接是可复用的。什么叫复用呢？复用就是一条连接可以供一台主机上的多个进程使用。
    歧义在“TCP”上，这个“粘包”跟TCP其实没关系。
    TCP保证的是数据流正确传输的机制，不管应用端怎么去用这个数据。
    这里的“粘包”其实是应用程序中没有处理好数据包分割，两个应用层的数据包粘在一块了。
    产生粘包的原因：
        1.由TCP连接复用造成的粘包问题
            如果没有复用一个连接只提供给端到端的两个进程使用，这是数据的传输方和发送方都是约定好了数据的格式的，但是多个进程使用一个TCP连接，此时多种不同结构的数据进到TCP的流式传输，边界分割肯定会出这样或者那样的问题
            如果利用tcp每次发送数据，就与对方建立连接，然后双方发送完一段数据后，就关闭连接，这样就不会出现粘包问题
        2.因为TCP默认会使用Nagle算法，此算法会导致粘包问题
            而Nagle算法主要做两件事
                1）只有上一个分组得到确认，才会发送下一个分组
                2）收集多个小分组，在一个确认到来时一起发送
            多个分组拼装为一个数据段发送出去，如果没有好的边界处理，在解包的时候会发生粘包问题
        3.数据包过大造成的粘包问题
            比如应用进程缓冲区的一条消息的字节的大小超过了发送缓冲区的大小，就有可能产生粘包问题
            因为消息已经被分割了，有可能一部分已经被发送出去了，对方已经接受了
            但是另外一部分可能刚放入套接口发送缓冲区里准备进一步发送，就直接导致接受的后一部分直接导致了粘包问题的出现
        4.流量控制，拥塞控制也可能导致粘包
        5.接收方不及时接收缓冲区的包，造成多个包接收。
            大多数人都是知道Nagle算法、接收方不及时处理两种情况造成的粘包问题，其他几种情况也是非常常见的
    粘包问题如何处理？
        1.Nagle算法问题导致的，需要结合应用场景适当关闭该算法。
        2.其他几种情况的处理方法主要分两种：
            尾部标记序列。通过特殊标识符表示数据包的边界，例如\n\r，\t，或者一些隐藏字符。
            头部标记分步接收。在TCP报文的头部加上表示数据长度。
            应用层发送数据时定长发送。

TCP如何保证可靠性：
    一、字节编号机制
        编号机制很好理解，就是给TCP的数据段里面的数据部分 ，每个字节都进行编号。
        为什么需要编号？
        好说，就是为了更清楚的接收和发送。TCP数据是按序的，接收完之后按序组装好，才会交付给上层。
    二、数据段的确认机制
        也就是我们常常听到的确认应答机制，一问一答，保证问的问题，对方一定接收到，如果确实没有接收到就会重复去问。
        TCP确认应答就是每一个数据段发送都会收到接收端返回的一个确认号，收到的确认号表示该号前面的数据全部接收。
        1、TCP可以一次连续发送多个数据段
            TCP可以连续发送多个数据段，具体发送数据段的多少取决于对方返回的窗口大小。只要满足窗口大小可容纳，Negale 算法处于关闭状态就可以连续发送多个数据段。
        2、仅对连续接受的数据段进行确认
            假设你发送了数据段序号为101、201、301、401、501、601，接收端接收到了101、201、501，此时接收端只会返回201的确认，不会返回501确认，因为301和401还没接收到。当收到301和401之后才会返回501的确认(在不超时的情况下)。
        3、不连续序号的数据先缓存下来
            如上面的例子，接收端收到101、201、501，此时501不能被确认，因为有不连续的数据，但是501的会被缓存在本地，后面收到301、401立即返回501的确认。
    三、TCP的超时重传机制
        前面两条都是预防和减少出错，超时重传机制是保证TCP在传输过程中数据丢失了一个回复措施。因此超时重传机制是保证可靠性很重要的机制。
        每发送一个TCP数据段都会启动一个超时重传计时器（Retransmission Timer,RTT)。如果在计时器时间内没有收到确认应答号，会启动重传，重新发送该数据段。
        算法：
            这里面还有个点，TCP目前采用一种自适应的算法计算RTT值。
            RTT新  =  (1 - α) · RTT旧 + α · RTT新
            RFC 6298推荐的α值为1/8，即0.125
            TCP每发送一个数据段不是立刻把该数据段从缓冲区删除的，收到确认应答以后才会从发送队列丢掉。给定一个初始的RTT值，初始RTT值是6s，后面每次收到确认应答会进行一次计算，计算本次往返的时间和RTT波动，也就是RTT偏差。最终把RTT+RTT偏差得到新的RTT值。
        数据也不会被无限、反复地重发。
        达到一定重发次数之后，如果仍没有任何确认应答返回，就会判断为网络或对端主机发生了异常，强制关闭连接。
        并且通知应用通信异常强行终止。










shuffle：
    缓冲 排序 压缩 io 拷贝 归并排序

2、常见应用层协议和运输层、网络层协议，以及硬件如路由器之类在哪一层
    ​ 传输层：TCP 、UDP
    ​ 网络层：ICMP 、IP、路由器、防火墙
    ​ 数据链路层：网卡、网桥、交换机
    ​ 物理层：中继器、集线器

4、TCP可靠传输的保证，拥塞控制目的和过程

    ​ TCP通过：应用数据分割、对数据包进行编号（序列号）、校验和、确认应答、流量控制、拥塞控制、ARP协议、超时重传等措施保证数据的可靠传输；

    ​ 拥塞控制目的：为了防止过多的数据注入到网络中，避免网络中的路由器、链路过载

    ​ 拥塞控制过程：TCP发送发将维护一个拥塞窗口的状态变量，该变量随着网络拥塞程度动态变化，通过慢开始、拥塞避免等算法减少网络拥塞的发生。

5、TCP粘包现象原因和解决方法
    ​ TCP粘包是指：发送方发送的若干包数据到接收方接收时粘成一包
    ​ 发送方原因：
        ​ TCP默认使用Nagle算法（主要作用：减少网络中报文段的数量），而Nagle算法主要做两件事：
        ​ 只有上一个分组得到确认，才会发送下一个分组
        ​ 收集多个小分组，在一个确认到来时一起发送
        ​ Nagle算法造成了发送方可能会出现粘包问题

    ​ 接收方原因：
        ​ TCP接收到数据包时，并不会马上交到应用层进行处理，或者说应用层并不会立即处理。
         实际上， TCP将接收到的数据包保存在接收缓存里，然后应用程序主动从缓存读取收到的分组。
         这样一来，如果 TCP 接收数据包到缓存的速度大于应用程序从缓存中读取数据包的速度，多个包就会被缓存，应用程 序就有可能读取到多个首尾相接粘到一起的包。
        ​ 解决粘包问题：
        ​ 最本质原因在与接收对等方无法分辨消息与消息之间的边界在哪，通过使用某种方案给出边界，例如：
        发送定长包。如果每个消息的大小都是一样的，那么在接收对等方只要累计接收数据，直到数据等于一个定长的数值就将它作为一个消息。
        包尾加上\r\n标记。FTP协议正是这么做的。但问题在于如果数据正文中也含有\r\n，则会误判为消息的边界。
        包头加上包体长度。包头是定长的4个字节，说明了包体的长度。接收对等方先接收包体长度，依据包体长度来接收包体。




四次挥手过程：
    ​ 客户端——发送带有FIN标志的数据包——服务端，关闭与服务端的连接 ，客户端进入FIN-WAIT-1状态
    ​ 服务端收到这个 FIN，它发回⼀ 个 ACK，确认序号为收到的序号加1，服务端就进入了CLOSE-WAIT状态,客户端就进入FIN-WAIT-2状态
    ​ 服务端——发送⼀个FIN数据包——客户端，关闭与客户端的连接，服务端进入LAST-ACK
    ​ 客户端收到这个 FIN，发回 ACK 报⽂确认，并将确认序号设置为收到序号加1，客户端进入TIME-WAIT状态，服务端进入CLOSED状态


进程通信方式
    1.管道：管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有血缘关系的进程间使用。
           进程的亲缘关系通常是指父子进程关系
        缺点：
            1.管道的通信方式是效率低的，因此管道不适合进程间频繁地交换数据
            2.通信的数据是无格式的流并且大小受限，通信的方式是单向的

    2.消息队列：消息队列是由消息的链表，存放在内核中并由消息队列标识符标识。
        优点：
            消息队列克服了信息传递信息少，管道只能承载无格式字节流以及缓冲区大小受限等特点。
        缺点：
            消息队列不适合比较大数据的传输
            消息队列通信过程中，存在用户态与内核态之间的数据拷贝开销

    3.共享内存：共享内存就是映射一段能够被其它进程所访问的内存，这段共享内存由一个进程创建，但多个进程可以访问
              共享内存的机制，就是拿出一块虚拟地址空间来，映射到相同的物理内存中。
        优点：
             解决消息队列通信中用户态与内核态之间数据拷贝过程带来的开销
             共享内存是最快的IPC方式，它是针对进程内通信方式运行效率而专门设计。
             它往往与其它通信机制，如信号量配合使用，来实现进程间的同步和通信。
        缺点：多个进程同时修改同一个共享内存，可能发生覆盖，多进程竞争同个共享资源会造成数据的错乱。

    4.信号量：信号量是一个计数器，可以用来控制多进程对共享资源的访问，主要用于实现进程间的互斥与同步，而不是用于缓存进程间通信的数据。
            它常作为一种锁机制，防止某个进程正在访问共享资源时，其它进程也访问该资源。
            因此，主要作为进程间以及同一进程内不同线程之间的同步手段。
        用法：
            可以发现，信号初始化为 1，就代表着是互斥信号量，它可以保证共享内存在任何时刻只有一个进程在访问，这就很好的保护了共享内存。
            信号初始化为 0，就代表着是同步信号量，它可以保证进程 A 应在进程 B 之前执行。
            以用信号量来实现多进程同步的方式，我们可以初始化信号量为 0

    5.信号:
        信号是进程间通信机制中唯一的异步通信机制，信号可以在应用进程和内核之间直接交互
        内核也可以利用信号来通知用户空间的进程发生了哪些系统事件，信号事件的来源主要有硬件来源（如键盘 Cltr+C ）和软件来源（如 kill 命令）
        一旦有信号发生，进程有三种方式响应信号
            1. 执行默认操作、2. 捕捉信号、3. 忽略信号。
        有两个信号是应用进程无法捕捉和忽略的，即 SIGKILL 和 SEGSTOP，这是为了方便我们能在任何时候结束或停止某个进程。
        信号是 UNIX 系统最先开始使用的进程间通信机制，因为 Linux 是继承于 UNIX 的，所以 Linux 也支持信号机制，通过向一个或多个进程发送异步事件信号来实现，信号可以从键盘或者访问不存在的位置等地方产生；
        信号通过 shell 将任务发送给子进程。

    5.套接字：套接字也是一种进程通信机制，与其它通信机制不同的是，它可以用于不同设备间的进程通信
        作用：跨网络与不同主机上的进程之间通信，就需要 Socket 通信了

网络的性能指标
    1、「比特」：比特(bit)是计算机中数据量的单位，也是信息论中使用的信息量的单位。英文单词bit来源于binary digit，意思是一个“二进制数字”。网络技术中的速率指的是连接在计算机网络上的主机在数字信道上传送数据的速率，它也称为数据率(data rate)或比特率(bit rate)。

    2、「带宽」：在计算机网络中，带宽用来表示网络的通信线路传送数据的能力，因此网络带宽表示单位时间内从网络中的某一点到另一点所能通过的“最高数据率”。这种意义的带宽的单位是比特/秒。

    3、「吞吐量」：吞吐量(throughput)表示在单位时间内通过某个网络（或信道、接口）的数据量，他表示当前网络传输数据的能力。

    4、时延：
        1、「发送时延」：指主机或路由器发送数据帧所需要的时间，也就是从发送数据帧的第一个比特算起，到该帧的最后一个比特发送完毕所需要的时间。
        2、「传播时延」：指电磁波在信道中传播一定距离需要花费的时间。

    5、「时延带宽积」：时延带宽积表示链路可容纳的比特数，因此，链路的时延带宽积又称为以比特为单位的链路长度。

    6、「往返时间RTT」：往返时间RTT，表示从发送方发送数据开始，到发送方收到来自接收方的确认（接收方收到数据后便立即发送确认），总共经历的时间。往返时间一般就会包括分组在网络中的各种时延。

    7、「利用率」：利用率可以分为信道利用率和网络利用率两种。
        1.信道利用率指出某信道有百分之几的时间是被利用的（有数据通过）。完全空闲的信道的利用率是零。
        2.网络的利用率则是全网络的信道利用率的加权平均值。
        信道利用率并非越高越好，这是因为，根据排队论的理论，当某信道的利用率增大时，该信道引起的时延也会迅速增加。信道或网络的利用率过高会产生非常大的时延。

线程间的通信方式
    锁机制：包括互斥锁，条件变量，读写锁
        互斥锁提供了以排他方式防止数据结构被并发修改的问题
        读写锁运行多线程同时读共享数据，而对写操作是互斥的
        条件变量可以以原子方式阻塞进程，知道某个特定条件为真为止。对条件的测试是在互斥锁的保护下进行的，条件变量始终与互斥锁一起使用。
    信号量机制（Semaphore）：包括无名线程信号量和命名线程信号量
    信号机制（Signal）：类似进程间的信号处理，线程间的通信目的主要是用于线程同步，所以线程没有像进程通信中的用于数据交换的通信机制。


内存管理方式：
存管理方式：块式管理、页式管理、段式管理、段页式管理
    分段管理：
        ​在段式存储管理中，将程序的地址空间划分为若干段（segment）
        如代码段，数据段，堆栈段；这样每个进程有一个二维地址空间，相互独立，互不干扰。
        段式管理的优点是：没有内碎片（因为段大小可变，改变段大小来消除内碎片）。
        但段换入换出时，会产生外碎片（比如4k的段换5k的段，会产生1k的外碎片）
    分页管理：
        ​ 在页式存储管理中，将程序的逻辑地址划分为固定大小的页（page），而物理内存划分为同样大小的页框
         程序加载时，可以将任意一页放入内存中任意一个页框，这些页框不必连续，从而实现了离散分离。
         页式存储管理的优点是：没有外碎片（因为页的大小固定），但会产生内碎片（一个页可能填充不满）
    段页式管理：
        ​段⻚式管理机制结合了段式管理和⻚式管理的优点。
        简单来说段⻚式管理机制就是把主存先分成若⼲ 段，每个段⼜分成若⼲⻚，也就是说 段⻚式管理机制 中段与段之间以及段的内部的都是离散的。

分页系统是什么：
    分页就是说，将磁盘或者硬盘分为大小固定的数据块，叫做页，然后内存也分为同样大小的块，叫做页框
    当进程执行的时候，会将磁盘的页载入内存的某些页框中，并且正在执行的进程如果发生缺页中断也会发生这个过程
    页和页框都是由两个部分组成的，一个是页号或者页框号，一个是偏移量
    分页一般是有硬件来完成的，每个页都对应一个页框，它们的对应关系存放在一个叫做页表的数据结构中
    页号作为这个页表的索引，页框号作为页表的值。操作系统负责维护这个页表

JDK动态代理与cglib实现的区别
    java动态代理是利用反射机制生成一个实现代理接口的匿名类，在调用具体方法前调用InvokeHandler处理。
    cglib动态代理是利用asm开源包，对代理对象类的class文件加载进来，通过修改其字节码生成子类来处理。
    JDK动态代理只能对实现了接口的类生成代理，而不能针对类
    cglib是针对类实现代理，主要是对指定的类生成一个子类，覆盖其中的方法。因为是继承，所以该类或方法最好不要声明成final

谈谈序列化与反序列化
    序列化是指将对象转换为字节序列的过程，而反序列化则是将字节序列转换为对象的过程。
    Java对象序列化是将实现了Serializable接口的对象转换成一个字节序列，能够通过网络传输、文件存储等方式传输 ，传输过程中却不必担心数据在不同机器、不同环境下发生改变，也不必关心字节的顺序或其他任何细节，并能够在以后将这个字节序列完全恢复为原来的对象。


请你介绍一下，数据库的三个范式？
    第一范式（1NF）：强调的是列的原子性，即列不能够再分成其他几列。
    第二范式（2NF）：首先满足 1NF，另外包含两部分内容，一是表必须有一个主键；二是没有包含在主键中的列必须完全依赖于主键，而不能只依赖于主键的一部分。
    第三范式（3NF）：首先是满足2NF，另外非主键列必须直接依赖于主键，不能存在传递依赖。即不能存在：非主键列 A 依赖于非主键列 B，非主键列 B 依赖于主键的情况
    有时为了满足查询速度，可以有意识的让某些表有些冗余，这是为了提高整个数据库的性能，所以有些时候，不一定要拘泥于达到第三范式或bcn 范式，只要数据库的设计可以提高整个数据库的性能，这就是一个合理的数据库
死锁条件，解决方式。

    ​ 死锁是指两个或两个以上进程在执行过程中，因争夺资源而造成的下相互等待的现象；
    ​ 死锁的条件：
        ​ 互斥条件：进程对所分配到的资源不允许其他进程访问，若其他进程访问该资源，只能等待，直至占有该资源的进程使用完成后释放该资源；
        ​ 请求与保持条件：进程获得一定的资源后，又对其他资源发出请求，但是该资源可能被其他进程占有，此时请求阻塞，但该进程不会释放自己已经占有的资源
        ​ 非剥夺条件：进程已获得的资源，在未完成使用之前，不可被剥夺，只能在使用后自己释放
        ​ 循环等待条件：系统中若干进程组成环路，环路中每个进程都在等待相邻进程占用的资源
    ​ 解决方法：破坏死锁的任意一条件
        ​ 资源一次性分配，从而剥夺请求和保持条件
        ​ 可剥夺资源：即当进程新的资源未得到满足时，释放已占有的资源，从而破坏不可剥夺的条件
        ​ 资源有序分配法：系统给每类资源赋予一个序号，每个进程按编号递增的请求资源，释放则相反，从而破坏环路等待的条件

1、Java面向对象特性介绍、与C++区别
特性：封装、继承、多态
    ​ 封装：对抽象的事物抽象化成一个对象，并对其对象的属性私有化，同时提供一些能被外界访问属性的方法，这样一个对象便有存在的意义了；
    ​ 继承：在已存在类的基础上，建立新类并对其增加新的数据域或功能，同时该类可以复用父类的属性与功能，这种思路可以称为继承；通过使用继承能够方便地复用旧代码，减少不必要的代码量；
    ​ 多态：指程序中的某个引用变量，它所指向的具体类型以及该引用变量发出的方法调用，在编程时不能确定，要在程序运行并使用时由机器自己判别确定；实现多态的方式有两种方式，可以通过继承（多个⼦类对同⼀⽅法的重写）、也可以通过接⼝（实现接⼝并覆盖接⼝中同⼀⽅法）

fork函数的作用:
    调用fork(),当控制转移到内核中的fork代码后，内核开始做：
    1.分配新的内存块和内核数据结构给子进程。
    2.将父进程部分数据结构内容拷贝至子进程。
    3.将子进程添加到系统进程列表。
    4.fork返回开始调度器，调度。
Linux常用命令
    1.查看端口号：
        第一种：
            lsof -i:端口号
        第二种：
            netstat -nltp | grep 端口号
    2.

Linux内存回收机制
    1.内存回收
        内存资源紧张会导致内存回收和 OOM 杀死进程。
        内存回收，也就是系统释放掉可以回收的内存，比如缓存和缓冲区，就属于可回收内存。它们在内存管理中，通常被叫做文件页（File-backed Page）。
        大部分文件页，都可以直接回收，以后有需要时，再从磁盘重新读取就可以了。而那些被应用程序修改过，并且暂时还没写入磁盘的数据（也就是脏页），就得先写入磁盘，然后才能进行内存释放。
        除了文件页，应用程序动态分配的堆内存，也就是我们在内存管理中说到的匿名页（Anonymous Page）也可以被回收。（使用 Swap 回收）
    2.Swap 机制
        Swap 把这些不常访问的内存先写到磁盘中，然后释放这些内存，给其他更需要的进程使用。再次访问这些内存时，重新从磁盘读入内存就可以了。
        Swap 说白了就是把一块磁盘空间或者一个本地文件（以下讲解以磁盘为例），当成内存来使用。它包括换出和换入两个过程。
        所谓换出，就是把进程暂时不用的内存数据存储到磁盘中，并释放这些数据占用的内存。
        而换入，则是在进程再次访问这些内存的时候，把它们从磁盘读到内存中来。
        我们常见的笔记本电脑的休眠和快速开机的功能，也基于 Swap 。休眠时，把系统的内存存入磁盘，这样等到再次开机时，只要从磁盘中加载内存就可以。这样就省去了很多应用程序的初始化过程，加快了开机速度。
    3.总结
        在内存资源紧张时，Linux 通过直接内存回收和定期扫描的方式，来释放文件页和匿名页，以便把内存分配给更需要的进程使用。
        文件页的回收比较容易理解，直接清空，或者把脏数据写回磁盘后再释放。
        而对匿名页的回收，需要通过 Swap 换出到磁盘中，下次访问时，再从磁盘换入到内存中。


select和poll和epoll(fd为file discriptor文件描述符)
linux重要的概念：一切皆文件，每一个网络连接在Linux中都是以文件描述符fd存在的
        (1)select==>时间复杂度O(n)
            它仅仅知道了，有I/O事件发生了，却并不知道是哪那几个流（可能有一个，多个，甚至全部），我们只能无差别轮询所有流，找出能读出数据，或者写入数据的流，对他们进行操作。所以select具有O(n)的无差别轮询复杂度，同时处理的流越多，无差别轮询时间就越长。
            1、 单个进程可监视的fd数量被限制，即能监听端口的大小有限。
                  一般来说这个数目和系统内存关系很大，具体数目可以cat /proc/sys/fs/file-max察看。32位机默认是1024个。64位机默认是2048.
            2、 对socket进行扫描时是线性扫描，即采用轮询的方法，效率较低：
            3、需要维护一个用来存放大量fd的数据结构，这样会使得用户空间和内核空间在传递该结构时复制开销大

        (2)poll==>时间复杂度O(n)
            poll本质上和select没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态，但是它没有最大连接数的限制，原因是它是基于链表来存储的.
            优点：
                它没有最大连接数的限制，原因是它是基于链表来存储的
            缺点：
                1、需要轮询时间复杂度O(N)
                2、大量的fd的数组被整体复制于用户态和内核地址空间之间，而不管这样的复制是不是有意义。
                3、poll还有一个特点是“水平触发”，如果报告了fd后，没有被处理，那么下次poll时会再次报告该fd。

        (3)epoll==>时间复杂度O(1)
            epoll可以理解为event poll，不同于忙轮询和无差别轮询，epoll会把哪个流发生了怎样的I/O事件通知我们。
            所以我们说epoll实际上是事件驱动（每个事件关联上fd）的，此时我们对这些流的操作都是有意义的。（复杂度降低到了O(1)）
            虽然连接数有上限，但是很大，1G内存的机器上可以打开10万左右的连接，2G内存的机器可以打开20万左右的连接
            epoll两种模式:
                epoll有EPOLLLT和EPOLLET两种触发模式，LT是默认的模式，ET是“高速”模式。
                LT模式下，只要这个fd还有数据可读，每次 epoll_wait都会返回它的事件，提醒用户程序去操作
                在ET（边缘触发）模式中，它只会提示一次，直到下次再有数据流入之前都不会再提示了，无 论fd中是否还有数据可读
                关于LT，ET，有一端描述，LT和ET都是电子里面的术语：
                    ET是边缘触发，LT是水平触发，一个表示只有在变化的边际触发，一个表示在某个阶段都会触发
                注意点:
                    1.所以在ET模式下，read一个fd的时候一定要把它的buffer读光，也就是说一直读到read的返回值小于请求值，或者 遇到EAGAIN错误
                    2.还有一个特点是，epoll使用“事件”的就绪通知方式，通过epoll_ctl注册fd，一旦该fd就绪，内核就会采用类似callback的回调机制来激活该fd，epoll_wait便可以收到通知

            epoll优点:
                1、没有最大并发连接的限制，能打开的FD的上限远大于1024（1G的内存上能监听约10万个端口）
                2、效率提升，不是轮询的方式，不会随着FD数目的增加效率下降。只有活跃可用的FD才会调用callback函数；
                    即Epoll最大的优点就在于它只管你“活跃”的连接，而跟连接总数无关，因此在实际的网络环境中，Epoll的效率就会远远高于select和poll。
                3、内存拷贝，使用共享内存，利用mmap()文件映射内存加速与内核空间的消息传递；即epoll使用mmap减少复制开销。

select、poll、epoll 区别总结：
            select/poll的几大缺点：
                1、每次调用select/poll，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大
                2、同时每次调用select/poll都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大
                3、针对select支持的文件描述符数量太小了，默认是1024
                4.select返回的是含有整个句柄的数组，应用程序需要遍历整个数组才能发现哪些句柄发生了事件；
                5.select的触发方式是水平触发，应用程序如果没有完成对一个已经就绪的文件描述符进行IO操作，那么之后每次select调用还是会将这些文件描述符通知进程。
            相比select模型，poll使用链表保存文件描述符，因此没有了监视文件数量的限制，但其他三个缺点依然存在。
            select，poll，epoll都是IO多路复用的机制。
            I/O多路复用就通过一种机制，可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。
            但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的
            而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。
            epoll跟select都能提供多路I/O复用的解决方案。
            在现在的Linux内核里有都能够支持，其中epoll是Linux所特有，而select则应该是POSIX所规定，一般操作系统均有实现

IO多路复用和多线程的应用场景:
    1.多路复用适用于需要保持大量闲置（区别于计算密集型）长连接的业务场景，例如聊天室。
        这样的好处是能够避免不断的创建新线程，导致系统资源浪费。
        需要注意多路复用本质上是复用单线程的，回调函数的执行必然是有可能长时间阻塞的，所以如果涉及到耗时的计算密集型任务，则会大大降低系统处理其它连接的响应速度。
    2.线程池则适合短连接并发的情况，比如普通的web业务系统，Tomcat的Servlet容器默认选择就是线程池（虽然3.0后支持异步，但一般情况下不常使用）
    3.协程除了在用户态通过栈切换实现控制流的切换以外，还通常将多路复用和线程池结合起来。
        比如go语言内置的协程就是在多线程的基础上实现了一套调度策略，调度策略的实现建立在操作系统内核提供的IO多路复用技术之上，同时go语言参考计算机硬件情况自动将协程绑定在若干个系统线程之上，从而实现资源的高效率利用。

Redis中IO多路复用技术：
    I/O多路复用就通过一种机制，可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。
    redis 是一个单线程却性能非常好的内存数据库， 主要用来作为缓存系统。
    epoll的基础就是回调
    redis 采用网络IO多路复用技术来保证在多连接的时候， 系统的高吞吐量。
    redis的io模型主要是基于epoll实现的，不过它也提供了 select和kqueue的实现，默认采用epoll。
    为什么 Redis 中要使用 I/O 多路复用这种技术呢？
    1.
      首先，Redis 是跑在单线程中的，所有的操作都是按照顺序线性执行的，但是由于读写操作等待用户输入或输出都是阻塞的，所以 I/O 操作在一般情况下往往不能直接返回
      这会导致某一文件的 I/O 阻塞导致整个进程无法对其它客户提供服务，而 I/O 多路复用就是为了解决这个问题而出现的。

SSD和传统磁盘的比较：
    1、SSD与传统磁盘相比，第一是没有机械装置，第二是由磁介质改为了电介质。SSD没有传统磁盘的寻道时间和延迟时间。
    2、基于SSD的优化就是解决erase-before-write产生的写入放大的问题。
    3、优化原理：
    a、将sequential logging修改为In-page logging，避免对相同位置的反复擦写。
    b.通过缓存写入的方式将大量的in-place update随机写入合并为少量顺序写入。
    c.利用SSD随机读写能力高的特点，减少写增加读，从而达到整体性能的提升

外部排序：
    1.可以利用两两归并的思想，但是磁盘IO次数太多了
    2.可以利用K路归并的思想，每个数据的io次数可以降低为K次（K越大越好吗？当然不是，如果K越大，需要排序的代价越大）
    3.置换选择算法，尽量增大有序子串的长度，可以使用堆（最小堆）的数据结构，然后一次弹出堆顶的元素放在子串中，当子串满了就开启下一个子串，注意子串的长度最少会减少一半的数量


关于hive中Map join 时大表left join小表的问题
    1.在hive中，（启用Map join时） 大表left join小表，加载从右向左，所以小表会加载进内存，存储成map键值对，通过大表驱动小表，来进行join，即大表中的join字段作为key 来获取value进行join。
        在MySQL中，left join加载从左向右，即join左边的表会先加载进内存，与右边表进行join。
    2.mapjoin时要让小表进内存，大表进内存的话可能会因为过大的原因，导致mapjoin无法实现，从而转为reduce join。
    MySQL跟hive加载顺序不同，MySQL使用left join的时候需要将大表放在后面
    最先加载的是驱动表，最好是小表

Mysql Like的用法：
    like 匹配/模糊匹配，会与 % 和 _ 结合使用
    '%a'     //以a结尾的数据
    'a%'     //以a开头的数据
    '%a%'    //含有a的数据
    '_a_'    //三位且中间字母是a的
    '_a'     //两位且结尾字母是a的
    'a_'     //两位且开头字母是a的

数据质量如何保障：
        (1)和产品运营经验值校对
        (2)对hive元数据实施监控，例如行数，表大小，空缺值
        (3)对核心指标实施监控
        (4)关注数仓血缘关系
        (5)尽可能在聚合层生成通用指标，不要再最上层再计算指标

缓慢变换维度:
    https://blog.csdn.net/weixin_34055787/article/details/92168690
    维度可以根据变化剧烈程度主要分为无变化维度、缓慢变化维度和剧烈变化维度。例如一个人的相关信息，身份证号、姓名和性别等信息数据属于不变的部分，政治面貌和婚姻状态属于缓慢变化部分，而工作经历、工作单位和培训经历等在某种程度上属于急剧变化字段。
    对于剧烈变化维度，通常情况下都是一分为二进行处理的，把其中不常变动的部分单独抽出来作为一个维表，按照缓慢变化维方式进行处理；另外一部分也单独抽取出来，通常作为维度的属性进行处理。
    大多数维度表随时间的迁移是缓慢变化的。比如增加了新的产品，或者产品的ID号码修改了，或者产品增加了一个新的属性，此时，维度表就会被修改或者增加新的记录行。这样，在设计维度和使用维度的过程中，就要考虑到缓慢变化维度的处理。
    缓慢渐变维，即维度中的属性可能会随着时间发生改变，比如包含用户住址Address的DimCustomer维度，用户的住址可能会发生改变，进而影响业务统计精度，DimCustomer维度就是缓慢渐变维（SCD），对于SCD，处理方式通常有以下几种：
    Type 1：完全不记录历史变化信息，在ETL将数据载入SCD的时候，对于会产生变化的属性值直接覆盖，比如对于DimCustomer的Address，每次都会将新的地址update到该字段，因此这个SCD实际上总是最新的当前信息，却没能包含历史信息
    Type 2：通过添加记录来将每一次变化都记录到SCD中，每条记录都有两个字段（如Effective_start和Effective_end）表明该记录的有效期间，并且可以设定一个Active标志位字段，当该字段为True的时候表明这条记录是最新的状态，为False的时候表明该记录是历史记录，其有效期间可以通过Effective_start和Effective_end字段来查
    Type 3：通过对会发生变化的字段，添加相应的历史字段，来记录最近的变化而非全部变化。比如DimCustomer中有两个字段Address和Address_Old，第一个字段是用户的当前住址，后一个字段是用户之前一次的住址，显然，更久之前的信息就无法追溯了
    Type 4：除了一个记录当前信息的维度外，单独建立一个历史信息维度，该维度中需要包含有效期间字段（如Effective_start和Effective_end）
    Type 5：可以看到，对于Type 1/2/3，都是对于SCD中渐变属性的处理方式，而针对一个包含多字段的复杂的SCD，可能需要结合以上三种处理方式。比如对于DimCustomer中的用户联系方式属性email，如果业务上并不重要，那么这个字段可以采取Type 1的方式，即每次只保留最新的联络方式，覆盖原来的；假如业务中需要分析用户所在地Region，那么很可能需要用到Type 2，记录每一个Region的改变；而对于地址信息Address，可能并不需要追溯很久的变化，那么加一个Address_Old字段来记录上一次的住址就够了

主题域：
    关于主题：
        数据仓库中的数据是面向主题组织的，主题是在较高层次上将企业信息系统中的数据进行综合、归类和分析利用的一个抽象概念
        每一个主题基本对应一个宏观的分析领域。如财务分析就是一个分析领域，因此这个数据仓库应用的主题就为“财务分析”。
    关于主题域：
        主题域通常是联系较为紧密的数据主题的集合。
        可以根据业务的关注点，将这些数据主题划分到不同的主题域(也说是对某个主题进行分析后确定的主题的边界。)
    关于主题域的划分：
        主题域的确定必须由最终用户和数据仓库的设计人员共同完成的， 而在划分主题域时，大家的切入点不同可能会造成一些争论、重构等的现象，考虑的点可能会是下方的某些方面：
        1、按照业务或业务过程划分：比如一个靠销售广告位置的门户网站主题域可能会有广告域，客户域等，而广告域可能就会有广告的库存，销售分析、内部投放分析等主题；
        2、根据需求方划分：比如需求方为财务部，就可以设定对应的财务主题域，而财务主题域里面可能就会有员工工资分析，投资回报比分析等主题；
        3、按照功能或应用划分：比如微信中的朋友圈数据域、群聊数据域等，而朋友圈数据域可能就会有用户动态信息主题、广告主题等；
        4、按照部门划分：比如可能会有运营域、技术域等，运营域中可能会有工资支出分析、活动宣传效果分析等主题；

划分数据域：
    数据仓库是面向主题（数据综合、归类并进行分析利用的抽象）的应用。数据仓库模型设计除横向的分层外，通常也需要根据业务情况进行纵向划分数据域。数据域是联系较为紧密的数据主题的集合，是业务对象高度概括的概念层次归类，目的是便于数据的管理和应用。
    数据域是指面向业务分析，将业务过程或者维度进行抽象的集合。
    数据域可以按照用户企业的部门划分，也可以按照业务过程或者业务板块中的功能模块进行划分。


Scala高阶函数是什么：
    高阶函数: 将其他函数作为参数或其结果是函数的函数
Scala柯里化：
    柯里化：柯里化(Currying)指的是将原来接受两个参数的函数变成新的接受一个参数的函数的过程

ArrayList和LinkedList的区别：
    1. 是否保证线程安全： ArrayList 和 LinkedList 都是不同步的，也就是不保证线程安全；
    2. 底层数据结构： Arraylist 底层使用的是Object数组；LinkedList 底层使用的是双向链表数据结构（JDK1.6之前为循环链表，JDK1.7取消了循环。注意双向链表和双向循环链表的区别，下面有介绍到！）
    3. 插入和删除是否受元素位置的影响：
        ① ArrayList 采用数组存储，所以插入和删除元素的时间复杂度受元素位置的影响。 比如：执行add(E e)方法的时候， ArrayList 会默认在将指定的元素追加到此列表的末尾，这种情况时间复杂度就是O(1)。但是如果要在指定位置 i 插入和删除元素的话（add(int index, E element)）时间复杂度就为 O(n-i)。因为在进行上述操作的时候集合中第 i 和第 i 个元素之后的(n-i)个元素都要执行向后位/向前移一位的操作。
        ② LinkedList 采用链表存储，所以插入，删除元素时间复杂度不受元素位置的影响，都是近似 O（1）而数组为近似 O（n）。
    4. 是否支持快速随机访问： LinkedList 不支持高效的随机元素访问，而 ArrayList 支持。
        快速随机访问就是通过元素的序号快速获取元素对象(对应于get(int index)方法)。
    5. 内存空间占用： ArrayList的空 间浪费主要体现在在list列表的结尾会预留一定的容量空间，而LinkedList的空间花费则体现在它的每一个元素都需要消耗比ArrayList更多的空间（因为要存放直接后继和直接前驱以及数据）


GC分代年龄为什么最大为15？
    因为Object Header采用4个bit位来保存年龄，4个bit位能表示的最大数就是15！
closed过多：
    1.导致无法开启新的连接，因为端口是有限的
    2.fd(文件描述符)不够

GC:
    1、为什么新生代都是复制算法
        因为新生代对象生存时间比较短，80%都是要回收的对象，采用标记-清除算法则内存空间碎片化严重，采用复制算法可以灵活高效，且便与整理空间。

    2、老年代都是标记整理算法
        标记整理算法解决来标记-清除算法的内存碎片化的问题，又解决了复制算法的两个Survivor区的问题，因为老年代的空间比较大，不可能采用复制算法，特别占用内存空间，

    3、为什么要设置两个Survivor区
        首先看下复制算法：Survivor区，一块叫From，一块叫To，对象存在Eden和From块。当进行GC时，Eden存活的对象全移到To块，而From中，存活的对象按年龄值确定去向，当达到一定值（年龄阈值，通过-XX:MaxTenuringThreshold可设置）的对象会移到年老代中，没有达到值的复制到To区，经过GC后，Eden和From被清空。之后，From和To交换角色，新的From即为原来的To块，新的To块即为原来的From块，且新的To块中对象年龄加1。永远有一个survivor space是空的，另一个非空的survivor space无碎片。

    因为都进入老年代了，存活率也一定很高啊~那要是开辟一个s1 s2岂不是太太太浪费？所以直接上标记清楚和定期的标记整理，为什么是这样呢？因为 直接的标记清除算法会导致内存碎片化，因此就引入了标记整理算法，该算法执行完毕后，存活的对象会按序放置，移动对象的内存地址（重点），来解决碎片化，但是执行时间较长
    时间换空间与空间换时间


三范式
    1NF:字段不可分;
    2NF:有主键，非主键字段依赖主键;
    3NF:非主键字段不能相互依赖;

    解释:
    1NF:原子性 字段不可再分,否则就不是关系数据库;
    2NF:唯一性 一个表只说明一个事物;
    3NF:每列都与主键有直接关系，不存在传递依赖;
    反三范式：
        没有冗余的数据库未必是最好的数据库，有时为了提高运行效率，提高读性能，就必须降低范式标准，适当保留冗余数据。具体做法是： 在概念数据模型设计时遵守第三范式，降低范式标准的工作放到物理数据模型设计时考虑。降低范式就是增加字段，减少了查询时的关联，提高查询效率，因为在数据库的操作中查询的比例要远远大于DML的比例。但是反范式化一定要适度，并且在原本已满足三范式的基础上再做调整的。

Linux五种io模型：
    https://www.cnblogs.com/lovejune/p/12547470.html

sparkSQL和spark core实现topN(分组topN)
    https://www.jianshu.com/p/bc977d23eb7f
    https://blog.csdn.net/luofazha2012/article/details/80636858

内核和CPU的区别:
    操作系统的内核（Kernel）属于操作系统层面，而 CPU 属于硬件
    CPU 主要提供运算，处理各种指令的能力。
    内核（Kernel）主要负责系统管理比如内存管理，它屏蔽了对硬件的操作。

操作系统：
    https://mp.weixin.qq.com/s/EIBwmgCNLtZKBFUYa4lcvg
    操作系统（Operating System，简称 OS）是管理计算机硬件与软件资源的程序，是计算机的基石。操作系统存在屏蔽了硬件层的复杂性
    操作系统的内核（Kernel）是操作系统的核心部分，它负责系统的内存管理，硬件设备的管理，文件系统的管理以及应用程序的管理。

内核态和用户态
    从宏观上来看，Linux操作系统的体系架构分为用户态和内核态（或者用户空间和内核）
    内核从本质上看是一种软件（程序）——控制计算机的硬件资源，并提供上层应用程序运行的环境
    用户态即上层应用程序的活动空间，应用程序的执行必须依托于内核提供的资源，包括CPU资源、存储资源、I/O资源等
    为了使上层应用能够访问到这些资源，内核必须为上层应用提供访问的接口：即系统调用
    1.操作系统cpu状态
        内核态（Kernel Mode）：运行操作系统程序，操作硬件
        用户态（User Mode）：运行用户程序
    2.指令划分
        特权指令：只能由操作系统使用、用户程序不能使用的指令。 举例：启动I/O 内存清零 修改程序状态字 设置时钟 允许/禁止终端 停机
        非特权指令：用户程序可以使用的指令。 举例：控制转移 算数运算 取数指令 访管指令（使用户程序从用户态陷入内核态）
    3.特权级别
        特权环：R0、R1、R2和R3
        R0相当于内核态，R3相当于用户态；
        不同级别能够运行不同的指令集合；
        因为操作系统的资源是有限的，如果访问资源的操作过多，必然会消耗过多的资源，而且如果不对这些操作加以区分，很可能造成资源访问的冲突。
        所以，为了减少有限资源的访问和使用冲突，Unix/Linux的设计哲学之一就是：对不同的操作赋予不同的执行等级，就是所谓特权的概念。
        简单说就是有多大能力做多大的事，与系统相关的一些特别关键的操作必须由最高特权的程序来完成。
        Intel的X86架构的CPU提供了0到3四个特权级，数字越小，特权越高，Linux操作系统中主要采用了0和3两个特权级，分别对应的就是内核态和用户态。
        运行于用户态的进程可以执行的操作和访问的资源都会受到极大的限制，而运行在内核态的进程则可以执行任何操作并且在资源的使用上没有限制。
        很多程序开始时运行于用户态，但在执行的过程中，一些操作需要在内核权限下才能执行，这就涉及到一个从用户态切换到内核态的过程。
        比如C函数库中的内存分配函数malloc()，它具体是使用sbrk()系统调用来分配内存，当malloc调用sbrk()的时候就涉及一次从用户态到内核态的切换，类似的函数还有printf()，调用的是wirte()系统调用来输出字符串，等等。
    4.区别
        用户态：处于用户态执行时，进程所能访问的内存空间和对象受到限制，其所处于占有的处理器是可被抢占的
        内核态：处于内核态执行时，则能访问所有的内存空间和对象，且所占有的处理器是不允许被抢占的。

以下三种情况会导致用户态到内核态的切换
    1.系统调用
        系统调用包括五种方式：进程(exit, fork)，文件(chmod, chown, open, fopen)，设备(read,write,ioctl)，信息(getxx)，通信（pipe,mmap）
        这是用户态进程主动要求切换到内核态的一种方式，用户态进程通过系统调用申请使用操作系统提供的服务程序完成工作。
        比如前例中fork()实际上就是执行了一个创建新进程的系统调用。
        而系统调用的机制其核心还是使用了操作系统为用户特别开放的一个中断来实现，例如Linux的int 80h中断。
        用户程序通常调用库函数，由库函数再调用系统调用，因此有的库函数会使用户程序进入内核态（只要库函数中某处调用了系统调用），有的则不会。
    2.异常
        当CPU在执行运行在用户态下的程序时，发生了某些事先不可知的异常，这时会触发由当前运行进程切换到处理此异常的内核相关程序中，也就转到了内核态，比如缺页异常。
    3.外围设备的中断
        当外围设备完成用户请求的操作后，会向CPU发出相应的中断信号，这时CPU会暂停执行下一条即将要执行的指令转而去执行与中断信号对应的处理程序，
        如果先前执行的指令是用户态下的程序，那么这个转换的过程自然也就发生了由用户态到内核态的切换。比如硬盘读写操作完成，系统会切换到硬盘读写的中断处理程序中执行后续操作等。
        这3种方式是系统在运行时由用户态转到内核态的最主要方式，其中系统调用可以认为是用户进程主动发起的，异常和外围设备中断则是被动的。
进程包含什么：
　　（a）进程描述信息：
　　du1、进程名或进程标识号：zhi每个进程dao都有唯一的进程名或进程标识号。在识别一个进程时，进程名或进程标识号代表该进程。
　　2、用户名或用户标识号：每个进程都隶属于某个用户，用户名或用户标识号有利于资源共享与保护。
　　3、家族关系：在有的系统中，进程之间互成家族关系。
　　（b）进程控制信息：
　　1、进程当前状态：说明进程当前处于何种状态。进程在活动期间可分为就绪态、执行态和等待状态。
　　2、进程优先级（priority）：进程优先级是选取进程占有处理机的重要依据。 与进程优先级有关的PCB表项还有：占有CPU时间，进程优先级偏移，占据内存时间等。
　　3、程序开始地址：程序开始地址规定该进程的程序以此地址开始执行。
　　4、各种计时信息：给出进程占有和利用资源的有关情况。
　　5、通信信息：通信信息用来说明该进程在执行过程中与别的进程所发生的信息交换情况。
　　6、资源管理信息：占用内存大小及其管理用数据结构指针。在某些复杂系统中，还有对换或覆盖用的有关信息。共享程序段大小及起始地址。输入输出设备的设备号，所要传送的数据长度、缓冲区地址、缓冲区长度及所用设备的有关数据结构指针等。指向文件系统的指针及有关标识等。进程可使用这些信息对文件系统进行操作。
　　（c）CPU现场保护结构：寄存器值（通用、程序计数器PC、状态PSW，地址包括栈指针）
MMU和TLB
    1.MMU是Memory Management Unit的缩写，中文名是内存管理单元，它是中央处理器（CPU）中用来管理虚拟存储器、物理存储器的控制线路，同时也负责虚拟地址映射为物理地址，以及提供硬件机制的内存访问授权，多用户多进程操作系统。
    2.TLB(Translation Lookaside Buffer)传输后备缓冲器是一个内存管理单元用于改进虚拟地址到物理地址转换速度的缓存。TLB是一个小的，虚拟寻址的缓存，其中每一行都保存着一个由单个PTE组成的块。如果没有TLB，则每次取数据都需要两次访问内存，即查页表获得物理地址和取数据。

session和cookie：
   session：
       Session是另一种记录客户状态的机制，不同的是Cookie保存在客户端浏览器中，而Session保存在服务器上。客户端浏览器访问服务器的时候，服务器把客户端信息以某种形式记录在服务器上。这就是Session。客户端浏览器再次访问时只需要从该Session中查找该客户的状态就可以了。
     　如果说Cookie机制是通过检查客户身上的“通行证”来确定客户身份的话，那么Session机制就是通过检查服务器上的“客户明细表”来确认客户身份。Session相当于程序在服务器上建立的一份客户档案，客户来访的时候只需要查询客户档案表就可以了。
   cookie:
       由于HTTP是一种无状态的协议，服务器单从网络连接上无从知道客户身份。怎么办呢？就给客户端们颁发一个通行证吧，每人一个，无论谁访问都必须携带自己通行证。这样服务器就能从通行证上确认客户身份了。这就是Cookie的工作原理。
    　　Cookie实际上是一小段的文本信息。客户端请求服务器，如果服务器需要记录该用户状态，就使用response向客户端浏览器颁发一个Cookie。客户端浏览器会把Cookie保存起来。当浏览器再请求该网站时，浏览器把请求的网址连同该Cookie一同提交给服务器。服务器检查该Cookie，以此来辨认用户状态。服务器还可以根据需要修改Cookie的内容。


Docker常用命令：
    安装和常用CLI：
    添加阿里云镜像：sudo yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
    安装命令：sudo yum install -y docker-ce docker-ce-cli containerd.io
    启动命令：sudo systemctl start docker
    添加当前用户到docker用户组：sudo usermod -aG docker $USER （需注销），newgrp docker （立即生效）
    Helloworld：docker run hello-world （本地没有镜像的话会自动从远端仓库pull）
    pull nginx 镜像：docker pull nginx（等效于nginx:latest）
    运行：docker run -【d】（后台运行不阻塞shell） 【-p 80:80】（指定容器端口映射，内部：外部） nginx
    查看正在运行：docker ps
    删除容器：docker rm -f <container id(不用打全，前缀区分)>
    进入bash：docker exec -it <container id(不用打全，前缀区分)> bash
    commit镜像：docker commit <container id(不用打全，前缀区分)> <name>
    查看镜像列表：docker images （刚才commit的镜像）
    使用运行刚才commit的镜像：docker run -d <name>
    使用Dockerfile构建镜像：docker build -t <name> <存放Dockerfile的文件夹>
    删除镜像：docker rmi <name>
    保存为tar：docker save <name> > <tar name>
    从tar加载：docker load < <tar name>

    一些启动参数：
    后台运行容器：-d
    容器内外端口映射：-p 内部端口号:外部端口号
    目录映射：-v 'dir name' : <dir>
    指定映像版本：<name>:<ver>

LinkedHashMap:
    		/**
		 * 实例化一个LinkedHashMap;
		 *
		 * LinkedHashMap的插入顺序和访问顺序;
		 * LinkedHashMap(int initialCapacity, float loadFactor, boolean accessOrder);
		 * 说明:
		 * 	当accessOrder为true时表示当前数据的插入读取顺序为访问顺序；
		 * 	当accessOrder为false时表示当前数据的插入读取顺序为插入顺序；
		 */
		Map<String,String> linkedHashMap = new LinkedHashMap<String,String>(0,0.75F,true); // 访问顺序;
//		Map<String,String> linkedHashMap = new LinkedHashMap<String,String>(0,0.75F,false); // 插入顺序;

MYSQL和ES和Hbase的应用场景：
    1.MySQL：
        关系型数据库，主要面向OLTP（联机事务处理），支持事务，支持二级索引，支持sql，支持主从、Group Replication架构模型（本文全部以Innodb为例，不涉及别的存储引擎）。
        MySQL中要提前定义表结构，也就是说表共有多少列（属性）需要提前定义好，并且同时需要定义好每个列所占用的存储空间。数据以行为单位组织在一起的，假如某一行的某一列没有数据，也需要占用存储空间。
    2.HBase：
        基于HDFS，支持海量数据读写（尤其是写），支持上亿行、上百万列的，面向列的分布式NoSql数据库。天然分布式，主从架构，不支持事务，不支持二级索引，不支持sql。
        HBase则是以列为单位存储数据，每一列就是一个key-value，HBase的表列（属性）不用提前定义，而且列可以动态扩展，比如人员信息表中需要添加一个新的“address”字段，MySQL需要提前alter表，HBase的话直接插入即可。
    3.
        ElasticSearch：ES是一款分布式的全文检索框架，底层基于Lucene实现，虽然ES也提供存储，检索功能，但许多开发者一直不认为ES是一款数据库，但是随着ES功能越来越强大，与数据库的界限也越来越模糊。天然分布式，p2p架构，不支持事务，采用倒排索引提供全文检索。
        ES比较灵活，索引中的field类型可以提前定义（定义mapping），也可以不定义，如果不定义，会有一个默认类型，不过出于可控性考虑，关键字段最好提前定义好。（Solr中必须提前定义好schema.xml文件）
    总结：
        MySQL行存储的方式比较适合OLTP（联机事务处理）业务。
        列存储的方式比较适合OLAP（联机分析处理）业务，而HBase采用了列族的方式平衡了OLTP和OLAP，支持水平扩展，如果数据量比较大、对性能要求没有那么高、并且对事务没有要求的话，HBase也是个不错的考虑。
        ES默认对所有字段都建了索引，所以比较适合复杂的检索或全文检索，例如在真格量化中对交易标的tick数据的检索。
    应用场景:
        MySQL在三款中最为成熟，而且支持事务，支持二级索引，容灾备份方案也最为成熟，所以线上核心业务Mysql是不二之选
        HBase因为其强大的写入能力和水平扩展能力，比较适合存储日志，用户行为等数据量比较大的数据，这种数据一般不涉及事务级别的读写，对二级索引的需求也不是很高。而且HBase的主键不像Mysql，往往是涉及到业务逻辑的，如果查询条件单一的话，可以把直接把需要查询的字段作为主键的一部分，类似MySQL的联合索引，来提供检索功能。
        ES现在不仅提供全文检索，还提供统计功能，并且提供的Restful接口非常好用，配上Kibana还可以进行图形化展示，第三方插件也很丰富。虽然ES可以水平扩展，但是考虑到ES的大部分检索都会检索该index的所有shard，如果单个index数据过大，性能多少也会受到影响，所以单个index的大小最好控制在一定的范围，比如存储用户行为日志的index，可以每隔一段时间归一次档，创建新的index，做到冷热分离。而且ES也可以作为MySQL或HBase的索引来使用，虽然Mysql也有索引功能，但是过多的索引往往会拖累MySQL的性能，并且线上MySQL数据库一般也不允许执行统计类的sql，这时可以用ES辅助实现统计，HBase因为只有主键检索，所以更需要二级索引的功能。



git：
    1.修改之前的commit记录
        1.如果是修改上一次的commit记录，则直接 git commit --amend -m "fix lastcommit"
        2.如果是修改之前以及commit的记录，则需要 git rebase -i HEAD~n, 然后将需要修改的commit记录的pick改为edit,然后执行git rebase --continue就修改完成了
    2.移动HEAD
        git reset HEAD~n
    3.撤销一个合并
        git reset --hard <合并前的SHA1>

