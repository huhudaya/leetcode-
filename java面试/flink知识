flink的checkpoint原理
1.简单的想法是在上游选择一个偏移量，然后让程序停下来，然后做一个快照，显然不可取
2.采用分布式快照，即使用一个barrier，barrier之前到来的元素，都保存在当前的checkpoint状态state中
  barrier之后到来的元素，都算作下一个checkpoint状态
  barrier可以采用广播的思路，将上游的barrier广播除去，然后下游的slot中的barrier到达之后，就做一个barrier，完成一个分布式快照，相当于是照相的时候，我们在不同时间进行拍照，然后对每个人进行一个拼接
  当上游有多个source怎么办？这样就有了多个barrier，此时就有一个barrier对齐的概念
  步骤：
    1.上游的每个source在某一时刻会发送一个barrier_id,单调递增，这个barrier_id会进行广播的操作
    2.下游的solt只有当上游所广播的barrier_id都收集到了才能保存状态！否则，在barrier之前达到的元素，都需要缓存在buffer_memory
3.checkpoint两阶段提交的流程
    1.开启一个kafka事务
    2.source插入barrier并且广播到下游operator
    3.遇见barrier的算子将状态存入状态后端
    4.sink收到barrier，保存当前状态，存入checkpoint，通知JOBmanager，并且开启下一阶段的事务，用于提交下一个检查点的事务
    5.JobManager收到所有任务的通知，发出确认信息，表示checkpoint完成
    6.sink收到JobManager的确认，正式提交这段时间的数据
    7.关闭Kafka事务，提交的数据可以正常消费了

数据湖和数据仓库：
    最大的区别是在与对 元数据/数据 开放程度不同
    数据湖中的数据对所有的计算引擎开放，广义的数据湖统一数据存储管理的能力（数据的统一存储，元数据的统一管理），又提供了计算能力，并且计算引擎是可以任意切换的
    计算：数据完整性，格式不统一，错误数据，重复数据，数据聚合，规则过滤

避免乱序
    在CDC的场景下，如果上游source是2，中间map有3个算子，下游sink并行度是2，那么如何避免乱序？
    在source到map可以使用hash,在map到sink也可以使用hash,避免直接使用rebalance

