flink的checkpoint原理
1.简单的想法是在上游选择一个偏移量，然后让程序停下来，然后做一个快照，显然不可取
2.采用分布式快照，即使用一个barrier，barrier之前到来的元素，都保存在当前的checkpoint状态state中
  barrier之后到来的元素，都算作下一个checkpoint状态
  barrier可以采用广播的思路，将上游的barrier广播出去，然后下游的slot中的barrier到达之后，就做一个barrier，完成一个分布式快照，相当于是照相的时候，我们在不同时间进行拍照，然后对每个人进行一个拼接
  当上游有多个source怎么办？这样就有了多个barrier，此时就有一个barrier对齐的概念
  步骤：
    1.上游的每个source在某一时刻会发送一个barrier_id,单调递增，这个barrier_id会进行广播的操作
    2.下游的solt只有当上游所广播的barrier_id都收集到了才能保存状态！否则，在barrier之前达到的元素，都需要缓存在buffer_memory
3.checkpoint两阶段提交的流程
    1.开启一个kafka事务
    2.source插入barrier并且广播到下游operator
    3.遇见barrier的算子将状态存入状态后端
    4.sink收到barrier，保存当前状态，存入checkpoint，通知JOBmanager，并且开启下一阶段的事务，用于提交下一个检查点的事务
    5.JobManager收到所有任务的通知，发出确认信息，表示checkpoint完成
    6.sink收到JobManager的确认，正式提交这段时间的数据
    7.关闭Kafka事务，提交的数据可以正常消费了
4.watermark：
    1.watermark实际上就是一个特殊的数据记录，必须是单调递增的，确保按事件时间驱动，watermark和数据的时间戳有关
    2.多并行度，watermark数据朝下游广播出去
    3.Long.MAX_VALUE表示事件时间的结束
    4.单个分区取最大值，多个分区取所有分区中的最小值

HADOOP负载均衡:
    副本存放机制：
        第一副本：放置在上传文件的DataNode上；如果是集群外提交，则随机挑选一台磁盘不太慢、CPU不太忙的节点上；
        第二副本：放置在于第一个副本不同的机架的节点上；
        第三副本：与第二个副本相同机架的不同节点上；
        如果还有更多的副本：随机放在节点中；
    负载均衡问题：
        Hadoop的HDFS集群非常容易出现机器与机器之间磁盘利用率不平衡的情况，例如：当集群内新增、删除节点，或者某个节点机器内硬盘存储达到饱和值。当数据不平衡时，Map任务可能会分配到没有存储数据的机器，这将导致网络带宽的消耗，也无法很好的进行本地计算。
        当HDFS负载不均衡时，需要对HDFS进行数据的负载均衡调整，即对各节点机器上数据的存储分布进行调整。从而，让数据均匀的分布在各个DataNode上，均衡IO性能，防止热点的发生。进行数据的负载均衡调整，必须要满足如下原则：
            数据平衡不能导致数据块减少，数据块备份丢失
            管理员可以中止数据平衡进程
            每次移动的数据量以及占用的网络资源，必须是可控的
            数据均衡过程，不能影响namenode的正常工作
        步骤：
            1.数据均衡服务（Rebalancing Server）首先要求 NameNode 生成 DataNode 数据分布分析报告,获取每个DataNode磁盘使用情况
            2.Rebalancing Server汇总需要移动的数据分布情况，计算具体数据块迁移路线图。数据块迁移路线图，确保网络内最短路径
            3.开始数据块迁移任务，Proxy Source Data Node复制一块需要移动数据块
            4.将复制的数据块复制到目标DataNode上
            5.删除原始数据块
            6.目标DataNode向Proxy Source Data Node确认该数据块迁移完成
            7.Proxy Source Data Node向Rebalancing Server确认本次数据块迁移完成。然后继续执行这个过程，直至集群达到数据均衡标准

仅一次语义：
    1.幂等写入
        一个操作，可以重复执行很多次，但是只导致一次的结果修改，后序重复执行就不起作用力了
    2.事务写入
        实现思想：构建的事务对应值checkpoint，知道checkpoint完成的时候，才将结果写入到sink系统中
        实现方式：预写日志，两阶段提交


数据湖和数据仓库：
    最大的区别是在与对 元数据/数据 开放程度不同
    数据湖中的数据对所有的计算引擎开放，广义的数据湖统一数据存储管理的能力（数据的统一存储，元数据的统一管理），又提供了计算能力，并且计算引擎是可以任意切换的
    计算：数据完整性，格式不统一，错误数据，重复数据，数据聚合，规则过滤

避免乱序
    在CDC的场景下，如果上游source是2，中间map有3个算子，下游sink并行度是2，那么如何避免乱序？
    在source到map可以使用hash,在map到sink也可以使用hash,避免直接使用rebalance

