flink处理增量数据文档:
https://blog.csdn.net/leaeason/article/details/107762692?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param

flink的checkpoint原理
1.简单的想法是在上游选择一个偏移量，然后让程序停下来，然后做一个快照，显然不可取
2.采用分布式快照，即使用一个barrier，barrier之前到来的元素，都保存在当前的checkpoint状态state中
  barrier之后到来的元素，都算作下一个checkpoint状态
  barrier可以采用广播的思路，将上游的barrier广播出去，然后下游的slot中的barrier到达之后，就做一个barrier
  完成一个分布式快照，相当于是照相的时候，我们在不同时间进行拍照，然后对每个人进行一个拼接
  当上游有多个source怎么办？这样就有了多个barrier，此时就有一个barrier对齐的概念
  步骤：
    1.上游的每个source在某一时刻会发送一个barrier_id,单调递增，这个barrier_id会进行广播的操作
    2.下游的solt只有当上游所广播的barrier_id都收集到了才能保存状态！否则，在barrier之前达到的元素，都需要缓存在buffer_memory
3.flinkcheckpoint两阶段提交的流程
    1.开启一个kafka事务
    2.source插入barrier并且广播到下游operator
    3.遇见barrier的算子将状态存入状态后端
    4.sink收到barrier，保存当前状态，存入checkpoint，通知JOBmanager，并且开启下一阶段的事务，用于提交下一个检查点的事务
    5.JobManager收到所有任务的通知，发出确认信息，表示checkpoint完成
    6.sink收到JobManager的确认，正式提交这段时间的数据
    7.关闭Kafka事务，提交的数据可以正常消费了
4.watermark：
    概念：
        1.wm是一种衡量event-time进展的机制，可以设定延迟触发
        2.wm是用来处理乱序事件的，而正确的处理乱序事件，需要结合上window的使用
        3.数据流中的wm表示timestamp小于wm的数据都已经到达了
        4.wm用来让程序自己平衡延迟和结果的正确性
        5.一般用当前分区中时间戳的最大值减去一个延迟时间作为当前分区的watermark
    传递(使用广播):
        1.watermark实际上就是一个特殊的数据记录，必须是单调递增的，确保按事件时间驱动，watermark和数据的时间戳有关
        2.多并行度，watermark数据上游朝下游广播出去，比如多个分区的wm分别为2，3，5，6，此时最小的wm为2，将2广播出去，然后下游收到这个广播的wm,表示2以前的数据都处理完成了
        3.Long.MAX_VALUE表示事件时间的结束
        4.单个分区取最大值，多个分区取所有分区中的最小值
        5.如果source的并行度弄成1会比较方便的调试程序


HADOOP负载均衡:
    副本存放机制：
        第一副本：放置在上传文件的DataNode上；如果是集群外提交，则随机挑选一台磁盘不太慢、CPU不太忙的节点上；
        第二副本：放置在于第一个副本不同的机架的节点上；
        第三副本：与第二个副本相同机架的不同节点上；
        如果还有更多的副本：随机放在节点中；
    负载均衡问题：
        Hadoop的HDFS集群非常容易出现机器与机器之间磁盘利用率不平衡的情况，例如：当集群内新增、删除节点，或者某个节点机器内硬盘存储达到饱和值。当数据不平衡时，Map任务可能会分配到没有存储数据的机器，这将导致网络带宽的消耗，也无法很好的进行本地计算。
        当HDFS负载不均衡时，需要对HDFS进行数据的负载均衡调整，即对各节点机器上数据的存储分布进行调整。从而，让数据均匀的分布在各个DataNode上，均衡IO性能，防止热点的发生。进行数据的负载均衡调整，必须要满足如下原则：
            数据平衡不能导致数据块减少，数据块备份丢失
            管理员可以中止数据平衡进程
            每次移动的数据量以及占用的网络资源，必须是可控的
            数据均衡过程，不能影响namenode的正常工作
        步骤：
            1.数据均衡服务（Rebalancing Server）首先要求 NameNode 生成 DataNode 数据分布分析报告,获取每个DataNode磁盘使用情况
            2.Rebalancing Server汇总需要移动的数据分布情况，计算具体数据块迁移路线图。数据块迁移路线图，确保网络内最短路径
            3.开始数据块迁移任务，Proxy Source Data Node复制一块需要移动数据块
            4.将复制的数据块复制到目标DataNode上
            5.删除原始数据块
            6.目标DataNode向Proxy Source Data Node确认该数据块迁移完成
            7.Proxy Source Data Node向Rebalancing Server确认本次数据块迁移完成。然后继续执行这个过程，直至集群达到数据均衡标准

仅一次语义：
    1.幂等写入
        一个操作，可以重复执行很多次，但是只导致一次的结果修改，后序重复执行就不起作用力了
    2.事务写入
        实现思想：构建的事务对应值checkpoint，知道checkpoint完成的时候，才将结果写入到sink系统中
        实现方式：预写日志，两阶段提交


数据湖和数据仓库：
    最大的区别是在与对 元数据/数据 开放程度不同
    数据湖中的数据对所有的计算引擎开放，广义的数据湖统一数据存储管理的能力（数据的统一存储，元数据的统一管理），又提供了计算能力，并且计算引擎是可以任意切换的
    计算：数据完整性，格式不统一，错误数据，重复数据，数据聚合，规则过滤

避免乱序
    在CDC的场景下，如果上游source是2，中间map有3个算子，下游sink并行度是2，那么如何避免乱序？
    在source到map可以使用hash,在map到sink也可以使用hash,避免直接使用rebalance


flink中并行度和task的关系
https://blog.csdn.net/qq_34897849/article/details/103931374
    Flink 中每一个 TaskManager 都是一个JVM进程，它可能会在独立的线程上执行一个或多个 subtask
    为了控制一个 TaskManager 能接收多少个 task， TaskManager 通过 task slot 来进行控制（一个 TaskManager 至少有一个 slot）
    slot 主要隔离内存，cpu 是slot之间共享的。也就是说4核的机器 ，内存足够，可以把slot设置为8。最多能同时运行8个任务。建议一个核心数分配一个slot
    这种图中 source、map 合成的task的并行度为6
keyby 、window、apply合成的task的并行度为6
sink的并行度为1
总共有13个task
但是不是需要13个slot才能满足这个并行度的要求

Flink-sql解析过程
    基于此，一次完整的SQL解析过程如下：
    用户使用对外提供Stream SQL的语法开发业务应用
    用calcite对StreamSQL进行语法检验，语法检验通过后，转换成calcite的逻辑树节点；最终形成calcite的逻辑计划
    采用Flink自定义的优化规则和calcite火山模型、启发式模型共同对逻辑树进行优化，生成最优的Flink物理计划
    对物理计划采用janino codegen生成代码，生成用低阶API DataStream 描述的流应用，提交到Flink平台执行

五、Flink 是如何保证Exactly-once语义的？
    Flink通过实现两阶段提交和状态保存来实现端到端的一致性语义。 分为以下几个步骤：
    1.开始事务（beginTransaction）创建一个临时文件夹，来写把数据写入到这个文件夹里面
    2.预提交（preCommit）将内存中缓存的数据写入文件并关闭
    3.正式提交（commit）将之前写完的临时文件放入目标目录下。这代表着最终的数据会有一些延迟
    4.丢弃（abort）丢弃临时文件
        若失败发生在预提交成功后，正式提交前。可以根据状态来提交预提交的数据，也可删除预提交的数据。