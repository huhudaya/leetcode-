hashMap术语介绍：
https://blog.csdn.net/qq_27409289/article/details/92759730
死磕源码：https://mp.weixin.qq.com/s/UFeLHR4qtGYPODTiNJDmHQ
源码解析（这个棒！）:https://mp.weixin.qq.com/s/Zf6eBolmxKbMPwWKU_DDTQ
    桶： 就是hashmap的table数组

    bin: 就是挂在数组上的链表

    TreeNode: 红黑树

    capacity： table总容量

    MIN_TREEIFY_CAPACITY ：64   转化为红黑树table最小大小

    TREEIFY_THRESHOLD ：8  转化为红黑树的阈值

    loadFactor:  0.75  table扩容因子，当实际length大于等于 capacity*loadFactor时会进行扩容，并且扩容是按照2的整数次幂

 新添加的元素怎么确定数组即table索引:
    1.通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h >>> 16)，主要是从速度、功效、质量来考虑的，这么做可以在数组table的length比较小的时候，也能保证考虑到高低Bit都参与到Hash的计算中，同时不会有太大的开销。hashcode是一个32位的int值范围从‑2147483648到2147483648
    2.只要哈希函数映射得比较均匀松散，一般应用是很难出现碰撞的。但问题是一个40亿长度的数组，内存是放不下的。你想，HashMap扩容之前的数组初始大小才16。所以这个散列值是不能直接拿来用的。用之前还要先做对数组的长度取模运算，得到的余数才能用来访问数组下标。源码中模运算是在这个indexFor( )函数里完成,主要逻辑  h & (length-1）
    3.右位移16位，正好是32bit的一半，自己的高半区和低半区做异或，就是为了混合原始哈希码的高位和低位，以此来加大低位的随机性。而且混合后的低位掺杂了高位的部分特征，这样高位的信息也被变相保留下来。

 为什么hashMap数组长度要为2的整数倍:
    HashMap的数组长度要取2的整次幂,因为这样（数组长度‑1）正好相当于一个“低位掩码”。“与”操作的结果就是散列值的高位全部归零，只保留低位值，用来做数组下标访问。以初始长度16为例，16‑1=15。2进制表示是00000000 0000000000001111。和某散列值做“与”操作如下，结果就是截取了最低的四位值。
    接下来，我们分析下为什么哈希表的容量一定要是2的整数次幂。
    首先，length为2的整数次幂的话，h&(length-1)就相当于对length取模，这样便保证了散列的均匀，同时也提升了效率；
    其次，length为2的整数次幂的话，为偶数，这样length-1为奇数，奇数的最后一位是1，这样便保证了h&(length-1)的最后一位可能为0，也可能为1（这取决于h的值）
    即与后的结果可能为偶数，也可能为奇数，这样便可以保证散列的均匀性
    而如果length为奇数的话，很明显length-1为偶数，它的最后一位是0，这样h&(length-1)的最后一位肯定为0，即只能为偶数，这样任何hash值都只会被散列到数组的偶数下标位置上，这便浪费了近一半的空间，因此，length取2的整数次幂，是为了使不同hash值发生碰撞的概率较小，这样就能使元素在哈希表中均匀地散列。

 为什么hashMap初始长度为16
 总结： 1 减少hash碰撞
       2 提高map查询效率
       3 分配过小防止频繁扩容
       4 分配过大浪费资源
 扩容机制
    HashMap的扩容机制用的很巧妙，以最小的性能来完成扩容。
    扩容后的容量就变成了变成了之前容量的2倍，初始容量为16，所以经过rehash之后，元素的位置要么是在原位置，要么是在原位置再向高下标移动上次容量次数的位置，也就是说如果上次容量是16，下次扩容后容量变成了16+16，如果一个元素在下标为7的位置，下次扩容时，要不还在7的位置，要不在7+16的位置。
    我们在扩充HashMap的时候，不需要像JDK1.7的实现那样重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”。
    而hash值的高位是否为1，只需要和扩容后的长度做与操作就可以了，因为扩容后的长度为2的次幂，所以高位必为1，低位必为0，如10000这种形式，源码中有e.hash & oldCap来做到这个逻辑。
    这个设计确实非常的巧妙，既省去了重新计算hash值的时间。
    而且同时，由于新增的1bit是0还是1可以认为是随机的
    因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。
    这一块就是JDK1.8新增的优化点。有一点注意区别，JDK1.7中rehash的时候，旧链表迁移新链表的时候，如果在新表的数组索引位置相同，则链表元素会倒置，但是从上图可以看出，JDK1.8不会倒置。


TreeNodes占用空间是普通Nodes的两倍，所以只有当bin包含足够多的节点时才会转成TreeNodes，而是否足够多就是由TREEIFY_THRESHOLD的值决定的。当bin中节点数变少时，又会转成普通的bin。并且我们查看源码的时候发现，链表长度达到8就转成红黑树，当长度降到6就转成普通bin

当hashCode离散性很好的时候，树型bin用到的概率非常小，因为数据均匀分布在每个bin中，几乎不会有bin中链表长度会达到阈值。
但是在随机hashCode下，离散性可能会变差，然而JDK又不能阻止用户实现这种不好的hash算法，因此就可能导致不均匀的数据分布。
不过理想情况下随机hashCode算法下所有bin中节点的分布频率会遵循泊松分布，我们可以看到，一个bin中链表长度达到8个元素的概率为0.00000006，几乎是不可能事件。


Java GC算法:
分代垃圾回收机制，是基于这样一个事实：不同的对象的生命周期是不一样的。因此，不同生命周期的对象可以采取不同的回收算法，以便提高回收效率。我们将对象分为三种状态：年轻代、年老代、持久代。JVM将堆内存划分为 Eden、Survivor 和 Tenured/Old 空间。
　　1. 年轻代

　　所有新生成的对象首先都是放在Eden区。 年轻代的目标就是尽可能快速的收集掉那些生命周期短的对象，对应的是Minor GC，每次 Minor GC 会清理年轻代的内存，算法采用效率较高的复制算法，频繁的操作，但是会浪费内存空间。当“年轻代”区域存放满对象后，就将对象存放到年老代区域。

　　2. 年老代

　　在年轻代中经历了N(默认15)次垃圾回收后仍然存活的对象，就会被放到年老代中。因此，可以认为年老代中存放的都是一些生命周期较长的对象。年老代对象越来越多，我们就需要启动Major GC和Full GC(全量回收)，来一次大扫除，全面清理年轻代区域和年老代区域。

　　3. 持久代

　　用于存放静态文件，如Java类、方法等。持久代对垃圾回收没有显著影响。　
　
   ·Minor GC:

　　用于清理年轻代区域。Eden区满了就会触发一次Minor GC。清理无用对象，将有用对象复制到“Survivor1”、“Survivor2”区中(这两个区，大小空间也相同，同一时刻Survivor1和Survivor2只有一个在用，一个为空)

　　·Major GC：

　　用于清理老年代区域。

　　·Full GC：

　　用于清理年轻代、年老代区域。 成本较高，会对系统性能产生影响。
GC过程
    1、新创建的对象，绝大多数都会存储在Eden中，

    2、当Eden满了（达到一定比例）不能创建新对象，则触发垃圾回收（GC），将无用对象清理掉，

           然后剩余对象复制到某个Survivor中，如S1，同时清空Eden区

    3、当Eden区再次满了，会将S1中的不能清空的对象存到另外一个Survivor中，如S2，

          同时将Eden区中的不能清空的对象，也复制到S1中，保证Eden和S1，均被清空。

    4、重复多次(默认15次)Survivor中没有被清理的对象，则会复制到老年代Old(Tenured)区中，

    5、当Old区满了，则会触发一个一次完整地垃圾回收（FullGC），之前新生代的垃圾回收称为（minorGC）

清除(清理)垃圾的算法：
1.标记-清除算法（老年代）
    该算法分为“标记”和“清除”两个阶段: 首先标记出所有需要回收的对象(可达性分析), 在标记完成后统一清理掉所有被标记的对象.
    该算法会有两个问题：
    效率问题，标记和清除效率不高。
    空间问题: 标记清除后会产生大量不连续的内存碎片, 空间碎片太多可能会导致在运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集。
    所以它一般用于"垃圾不太多的区域，比如老年代"。
2.复制算法（新生代）
该算法的核心是将可用内存按容量划分为大小相等的两块, 每次只用其中一块, 当这一块的内存用完, 就将还存活的对象（非垃圾）复制到另外一块上面, 然后把已使用过的内存空间一次清理掉。
优点：不用考虑碎片问题，方法简单高效。
缺点：内存浪费严重。
现代商用VM的新生代均采用复制算法，但由于新生代中的98%的对象都是生存周期极短的，因此并不需完全按照1∶1的比例划分新生代空间，而是将新生代划分为一块较大的Eden区和两块较小的Survivor区(HotSpot默认Eden和Survivor的大小比例为8∶1), 每次只用Eden和其中一块Survivor。
当发生MinorGC时，将Eden和Survivor中还存活着的对象一次性地拷贝到另外一块Survivor上， 最后清理掉Eden和刚才用过的Survivor的空间。当Survivor空间不够用(不足以保存尚存活的对象)时，需要依赖老年代进行空间分配担保机制，这部分内存直接进入老年代。
复制算法的空间分配担保：
在执行Minor GC前, VM会首先检查老年代是否有足够的空间存放新生代尚存活对象, 由于新生代使用复制收集算法, 为了提升内存利用率, 只使用了其中一个Survivor作为轮换备份, 因此当出现大量对象在Minor GC后仍然存活的情况时, 就需要老年代进行分配担保, 让Survivor无法容纳的对象直接进入老年代, 但前提是老年代需要有足够的空间容纳这些存活对象.
但存活对象的大小在实际完成GC前是无法明确知道的, 因此Minor GC前, VM会先首先检查老年代连续空间是否大于新生代对象总大小或历次晋升的平均大小, 如果条件成立, 则进行Minor GC, 否则进行Full GC(让老年代腾出更多空间).
然而取历次晋升的对象的平均大小也是有一定风险的, 如果某次Minor GC存活后的对象突增,远远高于平均值的话,依然可能导致担保失败(Handle Promotion Failure, 老年代也无法存放这些对象了), 此时就只好在失败后重新发起一次Full GC(让老年代腾出更多空间).
3.标记-整理算法（老年代）
标记清除算法会产生内存碎片问题, 而复制算法需要有额外的内存担保空间, 于是针对老年代的特点, 又有了标记整理算法. 标记整理算法的标记过程与标记清除算法相同, 但后续步骤不再对可回收对象直接清理, 而是让所有存活的对象都向一端移动,然后清理掉端边界以外的内存.

如何判定对象为垃圾对象：
1.在JDK1.2之前，使用的是引用计数器算法。
    在对象中添加一个引用计数器，当有地方引用这个对象的时候，引用计数器的值就+1，当引用失效的时候，计数器的值就-1，当引用计数器被减为零的时候，标志着这个对象已经没有引用了，可以回收了！
2.可达性分析法
在主流商用语言(如Java、C#)的主流实现中, 都是通过可达性分析算法来判定对象是否存活的: 通过一系列的称为 GC Roots 的对象作为起点, 然后向下搜索; 搜索所走过的路径称为引用链/Reference Chain, 当一个对象到 GC Roots 没有任何引用链相连时, 即该对象不可达, 也就说明此对象是不可用的, 如下图:虽然E和F相互关联， 但它们到GC Roots是不可达的, 因此也会被判定为可回收的对象。




序列化和反序列化:
      当两个进程远程通信时，彼此可以发送各种类型的数据。 无论是何种类型的数据，都会以二进制序列的形式在网络上传送。比如，我们可以通过http协议发送字符串信息;我们也可以在网络上直接发送Java对象。发送方需要把这个Java对象转换为字节序列，才能在网络上传送;接收方则需要把字节序列再恢复为Java对象才能正常读取。

      把Java对象转换为字节序列的过程称为对象的序列化。把字节序列恢复为Java对象的过程称为对象的反序列化。

      对象序列化的作用有如下两种：

      1. 持久化： 把对象的字节序列永久地保存到硬盘上，通常存放在一个文件中，比如：休眠的实现。以后服务器session管理，hibernate将对象持久化实现。

      2. 网络通信：在网络上传送对象的字节序列。比如：服务器之间的数据通信、对象传递。



